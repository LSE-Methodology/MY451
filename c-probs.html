<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>MY451 Introduction to Quantitative Analysis</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This course is intended for those with little or no past training in quantitative methods. The course is an intensive introduction to some of the principles and methods of statistical analysis in social research. Topics covered in MY451 include descriptive statistics, basic ideas of inference and estimation, contingency tables and an introduction to linear regression models.">
  <meta name="generator" content="bookdown 0.1.5 and GitBook 2.6.7">

  <meta property="og:title" content="MY451 Introduction to Quantitative Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This course is intended for those with little or no past training in quantitative methods. The course is an intensive introduction to some of the principles and methods of statistical analysis in social research. Topics covered in MY451 include descriptive statistics, basic ideas of inference and estimation, contingency tables and an introduction to linear regression models." />
  <meta name="github-repo" content="kbenoit/coursepack-bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="MY451 Introduction to Quantitative Analysis" />
  
  <meta name="twitter:description" content="This course is intended for those with little or no past training in quantitative methods. The course is an intensive introduction to some of the principles and methods of statistical analysis in social research. Topics covered in MY451 include descriptive statistics, basic ideas of inference and estimation, contingency tables and an introduction to linear regression models." />
  

<meta name="author" content="Jouni Kuha">


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="c-tables.html">
<link rel="next" href="c-3waytables.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MY451 Introduction to Quantitative Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course information</a></li>
<li class="chapter" data-level="1" data-path="c-intro.html"><a href="c-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="c-intro.html"><a href="c-intro.html#s-intro-purpose"><i class="fa fa-check"></i><b>1.1</b> What is the purpose of this course?</a></li>
<li class="chapter" data-level="1.2" data-path="c-intro.html"><a href="c-intro.html#s-intro-definitions"><i class="fa fa-check"></i><b>1.2</b> Some basic definitions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="c-intro.html"><a href="c-intro.html#ss-intro-def-subj"><i class="fa fa-check"></i><b>1.2.1</b> Subjects and variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="c-intro.html"><a href="c-intro.html#ss-intro-def-vartypes"><i class="fa fa-check"></i><b>1.2.2</b> Types of variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="c-intro.html"><a href="c-intro.html#ss-intro-def-descr"><i class="fa fa-check"></i><b>1.2.3</b> Description and inference</a></li>
<li class="chapter" data-level="1.2.4" data-path="c-intro.html"><a href="c-intro.html#ss-intro-def-assoc"><i class="fa fa-check"></i><b>1.2.4</b> Association and causation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="c-intro.html"><a href="c-intro.html#s-intro-outline"><i class="fa fa-check"></i><b>1.3</b> Outline of the course</a></li>
<li class="chapter" data-level="1.4" data-path="c-intro.html"><a href="c-intro.html#s-intro-maths"><i class="fa fa-check"></i><b>1.4</b> The use of mathematics and computing</a><ul>
<li class="chapter" data-level="1.4.1" data-path="c-intro.html"><a href="c-intro.html#symbolic-mathematics-and-mathematical-notation"><i class="fa fa-check"></i><b>1.4.1</b> Symbolic mathematics and mathematical notation</a></li>
<li class="chapter" data-level="1.4.2" data-path="c-intro.html"><a href="c-intro.html#computing-1"><i class="fa fa-check"></i><b>1.4.2</b> Computing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c-descr1.html"><a href="c-descr1.html"><i class="fa fa-check"></i><b>2</b> Descriptive statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-intro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-examples"><i class="fa fa-check"></i><b>2.2</b> Example data sets</a></li>
<li class="chapter" data-level="2.3" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-1cat"><i class="fa fa-check"></i><b>2.3</b> Single categorical variable</a><ul>
<li class="chapter" data-level="2.3.1" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cat-distr"><i class="fa fa-check"></i><b>2.3.1</b> Describing the sample distribution</a></li>
<li class="chapter" data-level="2.3.2" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cat-tables"><i class="fa fa-check"></i><b>2.3.2</b> Tabular methods: Tables of frequencies</a></li>
<li class="chapter" data-level="2.3.3" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cat-charts"><i class="fa fa-check"></i><b>2.3.3</b> Graphical methods: Bar charts</a></li>
<li class="chapter" data-level="2.3.4" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cat-descriptives"><i class="fa fa-check"></i><b>2.3.4</b> Simple descriptive statistics</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-2cat"><i class="fa fa-check"></i><b>2.4</b> Two categorical variables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-tables"><i class="fa fa-check"></i><b>2.4.1</b> Two-way contingency tables</a></li>
<li class="chapter" data-level="2.4.2" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-cond"><i class="fa fa-check"></i><b>2.4.2</b> Conditional proportions</a></li>
<li class="chapter" data-level="2.4.3" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-assoc"><i class="fa fa-check"></i><b>2.4.3</b> Conditional distributions and associations</a></li>
<li class="chapter" data-level="2.4.4" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-descr"><i class="fa fa-check"></i><b>2.4.4</b> Describing an association using conditional proportions</a></li>
<li class="chapter" data-level="2.4.5" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-gamma"><i class="fa fa-check"></i><b>2.4.5</b> A measure of association for ordinal variables</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-1cont"><i class="fa fa-check"></i><b>2.5</b> Sample distributions of a single continuous variable</a><ul>
<li class="chapter" data-level="2.5.1" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cont-tab"><i class="fa fa-check"></i><b>2.5.1</b> Tabular methods</a></li>
<li class="chapter" data-level="2.5.2" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cont-graphs"><i class="fa fa-check"></i><b>2.5.2</b> Graphical methods</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-nums"><i class="fa fa-check"></i><b>2.6</b> Numerical descriptive statistics</a><ul>
<li class="chapter" data-level="2.6.1" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-nums-central"><i class="fa fa-check"></i><b>2.6.1</b> Measures of central tendency</a></li>
<li class="chapter" data-level="2.6.2" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-nums-variation"><i class="fa fa-check"></i><b>2.6.2</b> Measures of variation</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-2cont"><i class="fa fa-check"></i><b>2.7</b> Associations which involve continuous variables</a></li>
<li class="chapter" data-level="2.8" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-presentation"><i class="fa fa-check"></i><b>2.8</b> Presentation of tables and graphs</a></li>
<li class="chapter" data-level="2.9" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-app"><i class="fa fa-check"></i><b>2.9</b> Appendix: Country data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c-samples.html"><a href="c-samples.html"><i class="fa fa-check"></i><b>3</b> Samples and populations</a><ul>
<li class="chapter" data-level="3.1" data-path="c-samples.html"><a href="c-samples.html#s-samples-intro"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="c-samples.html"><a href="c-samples.html#s-samples-finpops"><i class="fa fa-check"></i><b>3.2</b> Finite populations</a></li>
<li class="chapter" data-level="3.3" data-path="c-samples.html"><a href="c-samples.html#s-samples-samples"><i class="fa fa-check"></i><b>3.3</b> Samples from finite populations</a></li>
<li class="chapter" data-level="3.4" data-path="c-samples.html"><a href="c-samples.html#s-samples-infpops"><i class="fa fa-check"></i><b>3.4</b> Conceptual and infinite populations</a></li>
<li class="chapter" data-level="3.5" data-path="c-samples.html"><a href="c-samples.html#s-samples-popdistrs"><i class="fa fa-check"></i><b>3.5</b> Population distributions</a></li>
<li class="chapter" data-level="3.6" data-path="c-samples.html"><a href="c-samples.html#s-samples-inference"><i class="fa fa-check"></i><b>3.6</b> Need for statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c-tables.html"><a href="c-tables.html"><i class="fa fa-check"></i><b>4</b> Statistical inference for two-way tables</a><ul>
<li class="chapter" data-level="4.1" data-path="c-tables.html"><a href="c-tables.html#s-tables-intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="c-tables.html"><a href="c-tables.html#s-tables-tests"><i class="fa fa-check"></i><b>4.2</b> Significance tests</a></li>
<li class="chapter" data-level="4.3" data-path="c-tables.html"><a href="c-tables.html#s-tables-chi2test"><i class="fa fa-check"></i><b>4.3</b> The <span class="math inline">\(\chi^{2}\)</span> test of independence</a><ul>
<li class="chapter" data-level="4.3.1" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-null"><i class="fa fa-check"></i><b>4.3.1</b> Hypotheses</a></li>
<li class="chapter" data-level="4.3.2" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-ass"><i class="fa fa-check"></i><b>4.3.2</b> Assumptions of a significance test</a></li>
<li class="chapter" data-level="4.3.3" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-stat"><i class="fa fa-check"></i><b>4.3.3</b> The test statistic</a></li>
<li class="chapter" data-level="4.3.4" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-sdist"><i class="fa fa-check"></i><b>4.3.4</b> The sampling distribution of the test statistic</a></li>
<li class="chapter" data-level="4.3.5" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-Pval"><i class="fa fa-check"></i><b>4.3.5</b> The <span class="math inline">\(P\)</span>-value</a></li>
<li class="chapter" data-level="4.3.6" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-conclusions"><i class="fa fa-check"></i><b>4.3.6</b> Drawing conclusions from a test</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="c-tables.html"><a href="c-tables.html#s-tables-summary"><i class="fa fa-check"></i><b>4.4</b> Summary of the <span class="math inline">\(\chi^{2}\)</span> test of independence</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="c-probs.html"><a href="c-probs.html"><i class="fa fa-check"></i><b>5</b> Inference for population proportions</a><ul>
<li class="chapter" data-level="5.1" data-path="c-probs.html"><a href="c-probs.html#s-probs-intro"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="c-probs.html"><a href="c-probs.html#s-probs-examples"><i class="fa fa-check"></i><b>5.2</b> Examples</a></li>
<li class="chapter" data-level="5.3" data-path="c-probs.html"><a href="c-probs.html#s-probs-distribution"><i class="fa fa-check"></i><b>5.3</b> Probability distribution of a dichotomous variable</a></li>
<li class="chapter" data-level="5.4" data-path="c-probs.html"><a href="c-probs.html#s-probs-pointest"><i class="fa fa-check"></i><b>5.4</b> Point estimation of a population probability</a></li>
<li class="chapter" data-level="5.5" data-path="c-probs.html"><a href="c-probs.html#s-probs-test1sample"><i class="fa fa-check"></i><b>5.5</b> Significance test of a single proportion</a><ul>
<li class="chapter" data-level="5.5.1" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-hypotheses"><i class="fa fa-check"></i><b>5.5.1</b> Null and alternative hypotheses</a></li>
<li class="chapter" data-level="5.5.2" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-teststatistic"><i class="fa fa-check"></i><b>5.5.2</b> The test statistic</a></li>
<li class="chapter" data-level="5.5.3" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-samplingd"><i class="fa fa-check"></i><b>5.5.3</b> The sampling distribution of the test statistic and <span class="math inline">\(P\)</span>-values</a></li>
<li class="chapter" data-level="5.5.4" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-conclusions"><i class="fa fa-check"></i><b>5.5.4</b> Conclusions from the test</a></li>
<li class="chapter" data-level="5.5.5" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-summary"><i class="fa fa-check"></i><b>5.5.5</b> Summary of the test</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="c-probs.html"><a href="c-probs.html#s-probs-1sampleci"><i class="fa fa-check"></i><b>5.6</b> Confidence interval for a single proportion</a><ul>
<li class="chapter" data-level="5.6.1" data-path="c-probs.html"><a href="c-probs.html#s-probs-1sampleci-intro"><i class="fa fa-check"></i><b>5.6.1</b> Introduction</a></li>
<li class="chapter" data-level="5.6.2" data-path="c-probs.html"><a href="c-probs.html#s-probs-1sampleci-calc"><i class="fa fa-check"></i><b>5.6.2</b> Calculation of the interval</a></li>
<li class="chapter" data-level="5.6.3" data-path="c-probs.html"><a href="c-probs.html#s-probs-1sampleci-int"><i class="fa fa-check"></i><b>5.6.3</b> Interpretation of confidence intervals</a></li>
<li class="chapter" data-level="5.6.4" data-path="c-probs.html"><a href="c-probs.html#ss-means-ci-vstests"><i class="fa fa-check"></i><b>5.6.4</b> Confidence intervals vs. significance tests</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="c-probs.html"><a href="c-probs.html#s-probs-2samples"><i class="fa fa-check"></i><b>5.7</b> Inference for comparing two proportions</a></li>
<li class="chapter" data-level="5.8" data-path="c-probs.html"><a href="c-probs.html#s-regression-dummies"><i class="fa fa-check"></i><b>5.8</b> Including categorical explanatory variables</a><ul>
<li class="chapter" data-level="5.8.1" data-path="c-probs.html"><a href="c-probs.html#ss-regression-dummies-def"><i class="fa fa-check"></i><b>5.8.1</b> Dummy variables</a></li>
<li class="chapter" data-level="5.8.2" data-path="c-probs.html"><a href="c-probs.html#ss-regression-dummies-example"><i class="fa fa-check"></i><b>5.8.2</b> A second example</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="c-probs.html"><a href="c-probs.html#s-regression-rest"><i class="fa fa-check"></i><b>5.9</b> Other issues in linear regression modelling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="c-3waytables.html"><a href="c-3waytables.html"><i class="fa fa-check"></i><b>6</b> Analysis of 3-way contingency tables</a></li>
<li class="chapter" data-level="7" data-path="c-more.html"><a href="c-more.html"><i class="fa fa-check"></i><b>7</b> More statistics…</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="7.1" data-path="appendix.html"><a href="appendix.html#c-class0"><i class="fa fa-check"></i><b>7.1</b> Computer classes</a><ul>
<li class="chapter" data-level="7.1.1" data-path="appendix.html"><a href="appendix.html#general-instructions"><i class="fa fa-check"></i><b>7.1.1</b> General instructions</a></li>
<li class="chapter" data-level="7.1.2" data-path="appendix.html"><a href="appendix.html#s-intro-SPSS"><i class="fa fa-check"></i><b>7.1.2</b> Introduction to SPSS</a></li>
<li class="chapter" data-level="7.1.3" data-path="appendix.html"><a href="appendix.html#ss-class1"><i class="fa fa-check"></i><b>7.1.3</b> WEEK 2 class: Descriptive statistics for categorical data, and entering data</a></li>
<li><a href="appendix.html#week-3-class-descriptive-statistics-for-continuous-variables"><span class="toc-section-number">7.1.4</span> WEEK 3 class:<br />
Descriptive statistics for continuous variables</a></li>
<li class="chapter" data-level="7.1.5" data-path="appendix.html"><a href="appendix.html#week-4-class-two-way-contingency-tables"><i class="fa fa-check"></i><b>7.1.5</b> WEEK 4 class: Two-way contingency tables</a></li>
<li class="chapter" data-level="7.1.6" data-path="appendix.html"><a href="appendix.html#week-5-class-inference-for-two-population-means"><i class="fa fa-check"></i><b>7.1.6</b> WEEK 5 class: Inference for two population means</a></li>
<li class="chapter" data-level="7.1.7" data-path="appendix.html"><a href="appendix.html#week-7-class-inference-for-population-proportions"><i class="fa fa-check"></i><b>7.1.7</b> WEEK 7 class: Inference for population proportions</a></li>
<li class="chapter" data-level="7.1.8" data-path="appendix.html"><a href="appendix.html#week-7-class-correlation-and-simple-linear-regression-1"><i class="fa fa-check"></i><b>7.1.8</b> WEEK 7 class: Correlation and simple linear regression 1</a></li>
<li class="chapter" data-level="7.1.9" data-path="appendix.html"><a href="appendix.html#week-8-class-simple-linear-regression-and-3-way-tables"><i class="fa fa-check"></i><b>7.1.9</b> WEEK 8 class: Simple linear regression and 3-way tables</a></li>
<li class="chapter" data-level="7.1.10" data-path="appendix.html"><a href="appendix.html#week-9-class-multiple-linear-regression"><i class="fa fa-check"></i><b>7.1.10</b> WEEK 9 class: Multiple linear regression</a></li>
<li class="chapter" data-level="7.1.11" data-path="appendix.html"><a href="appendix.html#week-10-class-review-and-multiple-linear-regression"><i class="fa fa-check"></i><b>7.1.11</b> WEEK 10 class: Review and Multiple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="appendix.html"><a href="appendix.html#c-disttables"><i class="fa fa-check"></i><b>7.2</b> Statistical tables</a><ul>
<li class="chapter" data-level="7.2.1" data-path="appendix.html"><a href="appendix.html#s-disttables-Z"><i class="fa fa-check"></i><b>7.2.1</b> Table of standard normal tail probabilities</a></li>
<li class="chapter" data-level="7.2.2" data-path="appendix.html"><a href="appendix.html#s-disttables-t"><i class="fa fa-check"></i><b>7.2.2</b> Table of critical values for <span class="math inline">\(t\)</span>-distributions</a></li>
<li class="chapter" data-level="7.2.3" data-path="appendix.html"><a href="appendix.html#s-disttables-chi2"><i class="fa fa-check"></i><b>7.2.3</b> Table of critical values for <span class="math inline">\(\chi^{2}\)</span> distributions</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/kbenoit/coursepack-bookdown/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MY451 Introduction to Quantitative Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="c-probs" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Inference for population proportions</h1>
<div id="s-probs-intro" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>In this chapter we still consider statistical analyses which involve only discrete, categorical variables. In fact, we now focus on the simplest case of all, that of <strong>dichotomous</strong> (binary) variables which have only two possible values. Four examples which will be used for illustration throughout this chapter are introduced in Section <a href="c-probs.html#s-probs-examples">5.2</a>. In the first two of them we consider a binary variable in a single population, while in the last two examples the question of interest involves a comparison of the distributions of the variable between two populations (<em>groups</em>).</p>
<p>The data for such analyses can be summarised in simple tables, the one-group case with a one-way table of two cells, and the two-group case with a <span class="math inline">\(2\times 2\)</span> contingency table. Here, however, we formulate the questions of interest slightly differently, with primary emphasis on the <em>probability</em> of one of the two values of the variable of interest. In the one-group case the questions of interest are then about the population value of a single probability, and in the two-group case about the comparison of the values of this probability between the two groups.</p>
<p>While we describe specific methods of inference for these cases, we also use them to introduce some further general elements of statistical inference:</p>
<ul>
<li><p>Population <strong>parameters</strong> of probability distributions.</p></li>
<li><p><strong>Point estimation</strong> of population parameters.</p></li>
<li><p>Hypotheses anout the parameters, and significance tests for them.</p></li>
<li><p><strong>Confidence intervals</strong> for population parameters.</p></li>
</ul>
<p>The comparisons in the two-group analyses again address questions about associations, now between the group and the dichotomous variable of interest. Here it will be useful to employ the terminology introduced in Section <a href="c-intro.html#ss-intro-def-assoc">1.2.4</a>, which distinguishes between the <strong>explanatory variable</strong> and the <strong>response variable</strong> in the association. Following a common convention, we will denote the explanatory variable by <span class="math inline">\(X\)</span> and the response variable by <span class="math inline">\(Y\)</span>. In the two-group cases of this chapter, <span class="math inline">\(X\)</span> will be the group (which is itself also binary) and <span class="math inline">\(Y\)</span> the binary variable whose probabilities we are interested in. We will use <span class="math inline">\(Y\)</span> to denote this binary variable also in the one-group examples.</p>
</div>
<div id="s-probs-examples" class="section level2">
<h2><span class="header-section-number">5.2</span> Examples</h2>
<p>The following four examples will be discussed in this chapter. Examples 5.1 and 5.2 concern only one group, while in Examples 5.3 and 5.4 two groups are to be compared. Table <a href="#tab:t-probex">5.1</a> shows basic sample statistics for the examples, together with the results of the significance tests and confidence intervals described later.</p>
<p><strong>Example 5.1</strong>: <strong>An EU referendum</strong></p>
<p>A referendum about joining the European Union was held in Finland on the 16th of October, 1994. Suppose that in an opinion poll conducted around October 4th (just before the beginning of postal voting), 330 of the <span class="math inline">\(n=702\)</span> respondents (47.0%) indicated that they would definitely vote Yes to joining the EU, 190 (27.1%) said they would definitely vote No, and 182 (25.9%) were still undecided<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>. Here we will consider the dichotomous variable with the values of Yes (330 respondents) versus No or Undecided (372 respondents, or 53.0%). The proportion of voters who definitely intend to vote Yes provides a lower bound for the proportion of Yes-votes in the referendum, even if all of the currently undecided voters eventually decided to vote No.</p>
<p><strong>Example 5.2</strong>: <strong>Evidence of possible racial bias in jury selection</strong></p>
<p>As part of an official inquiry into the extent of racial and gender bias in the justice system in the U.S. state of Pennsylvania<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a>, an investigation was made of whether people from minority racial groups were underrepresented in trial juries. One part of the assessment was a survey administered to all those called for the jury panel for criminal trials (from which the juries for actual trials will be selected) in Allegheny County, Pennsylvania (the city of Pittsburgh and its surrounding areas) between May and October, 2001. We will consider the dichotomous variable of whether a respondent to the survey identified his or her own race as Black (African American) or some other race category. Of the <span class="math inline">\(n=4950\)</span> respondents, 226 (4.57%) identified themselves as black. This will be compared to the the percentage of blacks in the whole population of people aged 18 and over (those eligible for jury service) in the county, which is 12.4% (this is a census estimate which will here be treated as a known population quantity, ignoring any possible census error in it).</p>
<table>
<tbody>
<tr class="odd">
<td><strong>One sample</strong>             </td>
</tr>
</tbody>
</table>
<p>Example 5.1: Voting intention in an EU referendum <span class="math inline">\(n\)</span> Yes <span class="math inline">\(\hat{\pi}\)</span> <span class="math inline">\(\pi_{0}\)</span> <span class="math inline">\(z\)</span> <span class="math inline">\(P\)</span> 95% CI 702 330 0.470 0.5 <span class="math inline">\(-1.59\)</span> 0.112 (0.433; 0.507)</p>
<p>Example 5.2: Race of members of jury panel <span class="math inline">\(n\)</span> Black <span class="math inline">\(\hat{\pi}\)</span> <span class="math inline">\(\pi_{0}\)</span> <span class="math inline">\(z\)</span> <span class="math inline">\(P\)</span> 95% CI 4950 226 0.0457 0.124 <span class="math inline">\(-16.71\)</span> <span class="math inline">\(&lt;0.001\)</span> (0.040; 0.052)</p>
<p><strong>Two Independent samples</strong> Example 5.3: Polio diagnoses in a vaccine trial <span class="math inline">\(n\)</span> Yes <span class="math inline">\(\hat{\pi}\)</span> Diff. (<span class="math inline">\(\hat{\Delta}\)</span>) <span class="math inline">\(z\)</span> <span class="math inline">\(P\)</span> 95% CI Control group 201,229 142 0.000706<br />
(placebo)<br />
Treatment group 200,745 57 0.000284 <span class="math inline">\(-0.000422\)</span> <span class="math inline">\(-6.01\)</span> <span class="math inline">\(&lt;0.001\)</span> <span class="math inline">\((-0.000560;\)</span> (vaccine) <span class="math inline">\(-0.000284)\)</span></p>
<p>Example 5.4: Optimistic about young people’s future <span class="math inline">\(n\)</span> Yes <span class="math inline">\(\hat{\pi}\)</span> Diff. (<span class="math inline">\(\hat{\Delta}\)</span>) <span class="math inline">\(z\)</span> <span class="math inline">\(P\)</span> 95% CI Negative question 921 257 0.279<br />
Positive question 929 338 0.364 0.085 3.92 <span class="math inline">\(&lt;0.001\)</span> (0.043; 0.127) ————————————————————————————————————————————————————————–</p>
<p>: (#tab:t-probex) Examples of analyses of population proportions used in Chapter <a href="c-probs.html#c-probs">5</a>. In addition to sample sizes <span class="math inline">\(n\)</span> and proportions <span class="math inline">\(\hat{\pi}\)</span>, the table shows for the one-sample examples 5.1 and 5.2 the <span class="math inline">\(z\)</span>-test statistic for the hypothesis <span class="math inline">\(H_{0}: \pi=\pi_{0}\)</span>, its <span class="math inline">\(P\)</span>-value against a two-sided alternative, and a 95% confidence interval for <span class="math inline">\(\pi\)</span>. For the two-sample examples 5.3 and 5.4, the table shows the estimated between-group difference of proportions <span class="math inline">\(\hat{\Delta}\)</span>, the <span class="math inline">\(z\)</span>-test statistic for the hypothesis <span class="math inline">\(H_{0}: \Delta=0\)</span>, its <span class="math inline">\(P\)</span>-value against a two-sided alternative, and a 95% confidence interval for <span class="math inline">\(\Delta\)</span>.</p>
<p><strong>Example 5.3: The Salk polio vaccine field trial of 1954</strong></p>
<p>The first large-scale field trials of the “killed virus” polio vaccination developed by Dr. Jonas Salk were carried out in the U.S. in 1954<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>. In the randomized, double-blind placebo-control part of the trial, a sample of schoolchildren were randomly assigned to receive either three injections of the polio vaccine, or three injections of a placebo, inert saltwater which was externally indistinguishable from the real vaccine. The explanatory variable <span class="math inline">\(X\)</span> is thus the group (vaccine or “treatment” group vs. placebo or “control” group). The response variable <span class="math inline">\(Y\)</span> is whether the child was diagnosed with polio during the trial period (yes or no). There were <span class="math inline">\(n_{1}=201,229\)</span> children in the control group, and 142 of them were diagnosed with polio; in the treatment group, there were 57 new polio cases among <span class="math inline">\(n_{2}=200,745\)</span> children (in both cases only those children who received all three injections are included here). The proportions of cases of polio were thus <span class="math inline">\(0.000706\)</span> in the control group and <span class="math inline">\(0.000284\)</span> in the vaccinated group (i.e. 7.06 and 2.84 cases per 10,000 subjects, respectively).</p>
<p><strong>Example 5.4: Split-ballot experiment on acquiescence bias</strong></p>
<p>Survey questions often ask whether respondents agree or disagree with given statements on opinions or attitudes. <em>Acquiescence bias</em> means the tendency of respondents to agree with such statements, regardless of their contents. If it is present, we will overestimate the proportion of people holding the opinion corresponding to agreement with the statement. The data used in this example come from a study which examined acquiescence bias through a randomized experiment<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a>. In a survey carried out in Kazakhstan, the respondents were presented with a number of attitude statements, with four response categories: “Fully agree”, “Somewhat agree”, “Somewhat disagree”, and “Fully disagree”. Here we combine the first two and the last two, and consider the resulting dichotomous variable, with values labelled “Agree” and “Disagree”.</p>
<p>We consider one item from the survey, concerning the respondents’ opinions on the expectations of today’s young people. There were two forms of the question:</p>
<ul>
<li><p>“A young person today can expect little of the future”</p></li>
<li><p>“A young person today can expect much of the future”</p></li>
</ul>
<p>We will call these the “Negative” and “Positive” question respectively. Around half of the respondents were randomly assigned to receive the positive question, and the rest got the negative question. The explanatory variable <span class="math inline">\(X\)</span> indicates the type of question, with Negative and Positive questions coded here as 1 and 2 respectively. The dichotomous response variable <span class="math inline">\(Y\)</span> is whether the respondent gave a response which was optimistic about the future (i.e. agreed with the positive or disagreed with the negative question) or a pessimistic response. The sample sizes and proportions of optimistic responses in the two groups are reported in Table <a href="#tab:t-probex">5.1</a>. The proportion is higher when the question was worded positively, as we would expect if there was acquiescence bias. Whether this difference is statistically significant remains to be determined.</p>
</div>
<div id="s-probs-distribution" class="section level2">
<h2><span class="header-section-number">5.3</span> Probability distribution of a dichotomous variable</h2>
<p>The response variables <span class="math inline">\(Y\)</span> considered in this section have only two possible values. It is common to code them as 0 and 1. In our examples, we will define the values of the variable of interest as follows:</p>
<ul>
<li><p>Example 5.1: 1 if a person says that he or she will definitely vote Yes, and 0 if the respondent will vote No or is undecided</p></li>
<li><p>Example 5.2: 1 for black respondents and 0 for all others</p></li>
<li><p>Example 5.3: 1 if a child developed polio, 0 if not</p></li>
<li><p>Example 5.4: 1 if the respondent gave an optimistic response, 0 if not</p></li>
</ul>
<p>The population distribution of such a variable is completely specified by one number, the <strong>probability</strong> that a randomly selected member of the population will have the value <span class="math inline">\(Y=1\)</span> rather than 0. It can also be thought of as the <strong>proportion</strong> of units in the population with <span class="math inline">\(Y=1\)</span>; we will use the two terms interchangeably. This probability is denoted here <span class="math inline">\(\pi\)</span> (the lower-case Greek letter “pi”<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>). The value of <span class="math inline">\(\pi\)</span> is between 0 (no-one in the population has <span class="math inline">\(Y=1\)</span>) and 1 (everyone has <span class="math inline">\(Y=1\)</span>). Because <span class="math inline">\(Y\)</span> can have only two possible values, and the sum of probabilities must be one, the population probability of <span class="math inline">\(Y=0\)</span> is <span class="math inline">\(1-\pi\)</span>.</p>
<p> The probability distribution which corresponds to this kind of population distribution is the <em>Binomial distribution</em>. For later use, we note already here that the mean of this distribution is <span class="math inline">\(\pi\)</span> and its variance is <span class="math inline">\(\pi(1-\pi)\)</span>.</p>
<p>In Example 5.1, the population is that of eligible voters at the time of the opinion poll, and <span class="math inline">\(\pi\)</span> is the probability that a randomly selected eligible voter definitely intended to vote Yes. In Example 5.2, <span class="math inline">\(\pi\)</span> is the probability that a black person living in the county will be selected to the jury panel. In Example 5.3, <span class="math inline">\(\pi\)</span> is the probability (possibly different in the vaccinated and unvaccinated groups) that a child will develop polio, and in Example 5.4 it is the probability (which possibly depends on how the question was asked) that a respondent will give an optimistic answer to the survey question.</p>
<p>The probability <span class="math inline">\(\pi\)</span> is the <strong>parameter</strong> of the binomial distribution. In general, the parameters of a probability distribution are one or more numbers which fully determine the distribution. For example, in the analyses of Chapter <a href="c-tables.html#c-tables">4</a> we considered conditional distributions of a one variable in a contingency table given the other variable. Although we did not make use of this terminology there, these distributions also have their parameters, whcih are the probabilities of (all but one of) the categories of the response variable. Another case will be introduced in Chapter <a href="#c-means"><strong>??</strong></a>, where we consider a probability distribution for a continuous variable, and its parameters.</p>
</div>
<div id="s-probs-pointest" class="section level2">
<h2><span class="header-section-number">5.4</span> Point estimation of a population probability</h2>
<p>Questions and hypotheses about population distributions are usually most conveniently formulated in terms of the parameters of the distributions. For a binary variable <span class="math inline">\(Y\)</span>, this means that statistical inference will be focused on the probability <span class="math inline">\(\pi\)</span>.</p>
<p>The most obvious question about a parameter is “what is our best guess of the value of the parameter in the population?” The answer will be based on the information in the sample, using some sample statistic as the best guess or <strong>estimate</strong> of the population parameter. Specifically, this is a <strong>point estimate</strong>, because it is expressed as a single value or a “point”, to distinguish it from <em>interval</em> estimates defined later.</p>
<p>We denote a point estimate of <span class="math inline">\(\pi\)</span> by <span class="math inline">\(\hat{\pi}\)</span>. The “$;  ; $” or “hat” is often used to denote an estimate of a parameter indicated by the symbol under the hat; <span class="math inline">\(\hat{\pi}\)</span> is read as “pi-hat”. As <span class="math inline">\(\pi\)</span> for a binomial distribution is the population proportion of <span class="math inline">\(Y=1\)</span>, the obvious choice for a point estimate of it is the <em>sample</em> proportion of units with <span class="math inline">\(Y=1\)</span>. If we denote the <em>number</em> of such units by <span class="math inline">\(m\)</span>, the proportion is thus <span class="math inline">\(\hat{\pi}=m/n\)</span>, i.e. <span class="math inline">\(m\)</span> divided by the sample size <span class="math inline">\(n\)</span>. In Example 5.1, <span class="math inline">\(m=330\)</span> and <span class="math inline">\(n=702\)</span>, and <span class="math inline">\(\hat{\pi}=330/702=0.47\)</span>. This and the estimated proportions in the other examples are shown in Table <a href="#tab:t-probex">5.1</a>, in the two-sample examples 5.3 and 5.4 separately for the two groups.</p>
When <span class="math inline">\(Y\)</span> is coded with values 0 and 1, <span class="math inline">\(\hat{\pi}\)</span> is also equal to the sample mean of <span class="math inline">\(Y\)</span>, since
<span class="math display">\[\begin{equation}\bar{Y}=\frac{Y_{1}+Y_{2}+\dots+Y_{n}}{n}=
\frac{0+0+\dots+0+\overbrace{1+1+\dots+1}^{m \text{ ones}}}{n}=
\frac{m}{n}=\hat{\pi}.
\label{eq:pihat-as-ybar}\end{equation}\]</span>
</div>
<div id="s-probs-test1sample" class="section level2">
<h2><span class="header-section-number">5.5</span> Significance test of a single proportion</h2>
<div id="ss-probs-test1sample-hypotheses" class="section level3">
<h3><span class="header-section-number">5.5.1</span> Null and alternative hypotheses</h3>
A null hypothesis about a single population probability <span class="math inline">\(\pi\)</span> is of the form
<span class="math display">\[\begin{equation}H_{0}:\; \pi=\pi_{0}
\label{eq:H0p}\end{equation}\]</span>
<p>where <span class="math inline">\(\pi_{0}\)</span> is a given number which is either of specific interest or in some other sense a suitable benchmark in a given application. For example, in the voting example 5.1 we could consider <span class="math inline">\(\pi_{0}=0.5\)</span>, i.e. that the referendum was too close to call. In the jury example 5.2 the value of interest would be <span class="math inline">\(\pi_{0}=0.124\)</span>, the proportion of blacks in the general adult population of the county.</p>
An alternative but equivalent form of (\ref{eq:H0p}) is expressed in terms of the difference
<span class="math display">\[\begin{equation}\Delta=\pi-\pi_{0}
\label{eq:Dp}\end{equation}\]</span>
(<span class="math inline">\(\Delta\)</span> is the upper-case Greek letter “Delta”). Then (@(H0p)) can also be written as
<span class="math display">\[\begin{equation}H_{0}: \; \Delta=0,
\label{eq:H0p2}\end{equation}\]</span>
<p>i.e. that there is no difference between the true population probability and the hypothesised value <span class="math inline">\(\pi_{0}\)</span>. This version of the notation allows us later to draw attention to the similarities between different analyses in this chapter and in Chapter <a href="#c-means"><strong>??</strong></a>. In all of these cases the quantities of interest turn out to be differences of some kind, and the formulas for test statistics and confidence intervals will be of essentially the same form.</p>
The alternative hypothesis to the null hypothesis (\ref{eq:H0p2}) requires some further comments, because there are some new possibilities that did not arise for the <span class="math inline">\(\chi^{2}\)</span> test of independence in Chapter <a href="c-tables.html#c-tables">4</a>. For the difference <span class="math inline">\(\Delta\)</span>, we may consider two basic kinds of alternative hypotheses. The first is a <strong>two-sided alternative hypothesis</strong>
<span class="math display">\[\begin{equation}H_{a}: \; \Delta\ne 0
\label{eq:Hatwo}\end{equation}\]</span>
<p>(where “<span class="math inline">\(\ne\)</span>” means “not equal to”). This claims that the true value of the population difference <span class="math inline">\(\Delta\)</span> is some unknown value which is <em>not</em> 0 as claimed by the null hypothesis. With a two-sided <span class="math inline">\(H_{a}\)</span>, sample evidence that the true difference differs from 0 will be regarded as evidence against the null hypothesis, irrespective of whether it suggests that <span class="math inline">\(\Delta\)</span> is actually smaller or larger than 0 (hence the word “two-sided”). When <span class="math inline">\(\Delta=\pi-\pi_{0}\)</span>, this means that we are trying to assess whether the true probability <span class="math inline">\(\pi\)</span> is different from the claimed value <span class="math inline">\(\pi_{0}\)</span>, but without any expectations about whether <span class="math inline">\(\pi\)</span> might be smaller or larger than <span class="math inline">\(\pi_{0}\)</span>.</p>
The second main possibility is one of the two <strong>one-sided alternative hypotheses</strong>
<span class="math display">\[\begin{aligned}
H_{a}:&amp;&amp;  \Delta&gt; 0 \text{or}
\label{eq:Haonegt}
\\
H_{a}:&amp;&amp;  \Delta &lt; 0.
\label{eq:Haonelt}\end{aligned}\]</span>
<p>Such a hypothesis is only interested in values of <span class="math inline">\(\Delta\)</span> to one side of 0, either larger or smaller than it. For example, hypothesis (\ref{eq:Haonegt}) in the referendum example 5.1, with <span class="math inline">\(\pi_{0}=0.5\)</span>, is <span class="math inline">\(H_{a}:\; \pi&gt;0.5\)</span>, i.e. that the proportion who intend to vote Yes is <em>greater</em> than one half. Similarly, in the jury example 5.2, with <span class="math inline">\(\pi_{0}=0.124\)</span>, (\ref{eq:Haonelt}) is the hypothesis <span class="math inline">\(H_{a}:\; \pi&lt;0.124\)</span>, i.e. that the probability that an eligible black person is selected to a jury panel is <em>smaller</em> than the proportion of blacks in the general population.</p>
<p>Whether we choose to consider a one-sided or a two-sided alternative hypothesis depends largely on the research questions. In general, a one-sided hypothesis would be used when deviations from the null hypothesis only in one direction would be interesting and/or surprising. This draws on background information about the variables. A two-sided alternative hypothesis is neutral in this respect. Partly for this reason, two-sided hypotheses are in practice used more often than one-sided ones. Choosing a two-sided alternative hypothesis is not wrong even when a one-sided one could also be considered; this will simply lead to a more cautious (conservative) approach in that it takes stronger evidence to reject the null hypothesis when the alternative is two-sided than when it is one-sided. Such conservatism is typically regarded as a desirable feature in statistical inference (this will be discussed further in Section <a href="#ss-means-tests3-errors"><strong>??</strong></a>).</p>
<p>The two-sided alternative hypothesis (\ref{eq:Hatwo}) is clearly the logical opposite of the null hypothesis (\ref{eq:H0p2}): if <span class="math inline">\(\Delta\)</span> is not equal to 0, it must be “not equal” to 0. So a two-sided alternative hypothesis must correspond to a “point” null hypothesis (\ref{eq:H0p2}). For a one-sided alternative hypothesis, the same logic would seem to imply that the null hypothesis should also be one-sided: for example, <span class="math inline">\(H_{0}: \; \Delta\le 0\)</span> and <span class="math inline">\(H_{a}:\; \Delta&gt;0\)</span> would form such a logical pair. Often such “one-sided” null hypothesis is also closest to our research questions: for example, it would seem more interesting to try to test the hypothesis that the proportion of Yes-voters is less than or equal to 0.5 than that it is exactly 0.5. It turns out, however, that when the alternative hypothesis is, say, <span class="math inline">\(H_{a}: \Delta&gt;0\)</span>, the test will be the same when the null hypothesis is <span class="math inline">\(H_{0}: \; \Delta\le 0\)</span> as when it is <span class="math inline">\(H_{0}: \Delta= 0\)</span>, and rejecting or not rejecting one of them is equivalent to rejecting or not rejecting the other. We can thus here always take the null hypothesis to be technically of the form (\ref{eq:H0p2}), even if we are really interested in a corresponding “one-sided” null hypothesis. It is then only the alternative hypothesis which is explicitly either two-sided or one-sided.</p>
</div>
<div id="ss-probs-test1sample-teststatistic" class="section level3">
<h3><span class="header-section-number">5.5.2</span> The test statistic</h3>
The test statistic used to test hypotheses of the form (\ref{eq:H0p}) is the <strong>z-test statistic</strong>
<span class="math display">\[\begin{equation}z=
\frac{\hat{\Delta}}{\hat{\sigma}_{\hat{\Delta}}}=
\frac{\text{Estimate of the population difference $\Delta$}}
{\text{Estimated standard error
of the estimate of $\Delta$}}.
\label{eq:ttest-gen}\end{equation}\]</span>
<p>The statistic is introduced first in this form in order to draw attention to its generality. Null hypotheses in many ostensibly different situations can be formulated as hypotheses of the form (\ref{eq:H0p2}) about population differences of some kind, and each can be tested with the test statistic (\ref{eq:ttest-gen}). For example, all of the test statistics discussed in Chapters <a href="c-probs.html#c-probs">5</a>, <a href="#c-means"><strong>??</strong></a> and <a href="#c-regression"><strong>??</strong></a> of this course pack will be of this type (but the <span class="math inline">\(\chi^{2}\)</span> test statistic of Chapter <a href="c-tables.html#c-tables">4</a> is not). The principles of the use and interpretation of the test that are introduced in this section apply almost unchanged also in these other contexts, and only the exact formulas for calculating <span class="math inline">\(\hat{\Delta}\)</span> and <span class="math inline">\(\hat{\sigma}_{\hat{\Delta}}\)</span> will need to be defined separately for each of them. In some applications considered in Chapter <a href="#c-means"><strong>??</strong></a> the test statistic is typically called the <strong>t-test statistic</strong> instead of the <span class="math inline">\(z\)</span>-test statistic, but its basic idea is still the same.</p>
In (\ref{eq:ttest-gen}), <span class="math inline">\(\hat{\Delta}\)</span> denotes a sample estimate of <span class="math inline">\(\Delta\)</span>. For a test of a single proportion, this is
<span class="math display">\[\begin{equation}\hat{\Delta} = \hat{\pi}-\pi_{0},
\label{eq:Dhatp}\end{equation}\]</span>
<p>i.e. the difference between the sample proportion and <span class="math inline">\(\pi_{0}\)</span>. This is the core of the test statistic. Although the forms of the two statistics seem rather different, (\ref{eq:Dhatp}) contains the comparison of the observed and expected sample values that was also at the heart of the <span class="math inline">\(\chi^{2}\)</span> test statistic (\ref{eq:chi2}) in Chapter <a href="c-tables.html#c-tables">4</a>. Here the “observed value” is the sample estimate <span class="math inline">\(\hat{\pi}\)</span> of the probability parameter, “expected value” is the value <span class="math inline">\(\pi_{0}\)</span> claimed for it by the null hypothesis, and <span class="math inline">\(\hat{\Delta}=\hat{\pi}-\pi_{0}\)</span> is their difference. (Equivalently, we could also say that the expected value of <span class="math inline">\(\Delta=\pi-\pi_{0}\)</span> under the null hypothesis (\ref{eq:H0p2}) is 0, its observed value is <span class="math inline">\(\hat{\Delta}\)</span>, and <span class="math inline">\(\hat{\Delta}=\hat{\Delta}-0\)</span> is their difference.)</p>
<p>If the null hypothesis was true, we would expect the observed difference <span class="math inline">\(\hat{\Delta}\)</span> to be close to 0. If, on the other hand, the true <span class="math inline">\(\pi\)</span> was different from <span class="math inline">\(\pi_{0}\)</span>, we would expect the same to be true of <span class="math inline">\(\hat{\pi}\)</span> and thus <span class="math inline">\(\hat{\Delta}\)</span> to be different from 0. In other words, the difference <span class="math inline">\(\hat{\Delta}=\hat{\pi}-\pi_{0}\)</span> tends to be small (close to zero) when the null hypothesis is true, and large (far from zero) when it is not true, thus satisfying one of the requirements for a good test statistic that were stated at the beginning of Section <a href="c-tables.html#ss-tables-chi2test-sdist">4.3.4</a>. (Whether in this we count as “large” both large positive and large negative values, or just one or the other, depends on the form of the alternative hypothesis, as explained in the next section.)</p>
<p>The <span class="math inline">\(\hat{\sigma}_{\hat{\Delta}}\)</span> in (\ref{eq:ttest-gen}) denotes an estimate of the standard deviation of the sampling distribution of <span class="math inline">\(\hat{\Delta}\)</span>, which is also known as the estimated <strong>standard error</strong> of <span class="math inline">\(\hat{\Delta}\)</span>. For the test statistic (\ref{eq:ttest-gen}), it is evaluated under the null hypothesis. The concept of a standard error of an estimate will be discussed in more detail in Section <a href="#s-contd-clt"><strong>??</strong></a>. Its role in the test statistic is to provide an interpretable scale for the size of <span class="math inline">\(\hat{\Delta}\)</span>, so that the sampling distribution discussed in the next section will be of a convenient form.</p>
For a test of the hypothesis (\ref{eq:H0p}) about a single proportion, the estimated standard error under the null hypothesis is
<span class="math display">\[\begin{equation}\hat{\sigma}_{\hat{\Delta}} = \sqrt{\frac{\pi_{0}(1-\pi_{0})}{n}},
\label{eq:seDhatp}\end{equation}\]</span>
and the specific formula of the test statistic (\ref{eq:ttest-gen}) is then
<span class="math display">\[\begin{equation}z=\frac{\hat{\pi}-\pi_{0}}
{\sqrt{\pi_{0}(1-\pi_{0})/n}}.
\label{eq:ztestp}\end{equation}\]</span>
<p>This is the <strong>one-sample <span class="math inline">\(z\)</span>-test statistic for a population proportion</strong>.</p>
<p>In Example 5.1 we have <span class="math inline">\(\hat{\pi}=0.47\)</span>, <span class="math inline">\(\pi_{0}=0.5\)</span>, and <span class="math inline">\(n=702\)</span>, so <span class="math display">\[z=\frac{\hat{\pi}-\pi_{0}}{\sqrt{\pi_{0}(1-\pi_{0})/n}}=
\frac{0.47-0.50}{\sqrt{0.50\times(1-0.50)/702}}=-1.59.\]</span> Similarly, in Example 5.2 we have <span class="math inline">\(\hat{\pi}=0.0457\)</span>, <span class="math inline">\(\pi_{0}=0.124\)</span>, <span class="math inline">\(n=4950\)</span>, and <span class="math display">\[z=\frac{0.0457-0.124}{\sqrt{0.124\times(1-0.124)/4950}}
=
\frac{-0.0783}{\sqrt{0.10862/4950}}=-16.71.\]</span> Strangely, SPSS does not provide a direct way of calculating this value. However, since the formula (\ref{eq:ztestp}) is very simple, we can easily calculate it with a pocket calculator, after first using SPSS to find out <span class="math inline">\(\hat{\pi}\)</span>. This approach will be used in the computer classes.</p>
</div>
<div id="ss-probs-test1sample-samplingd" class="section level3">
<h3><span class="header-section-number">5.5.3</span> The sampling distribution of the test statistic and <span class="math inline">\(P\)</span>-values</h3>
<p>Like the <span class="math inline">\(\chi^{2}\)</span> test of Chapter <a href="c-tables.html#c-tables">4</a>, the <span class="math inline">\(z\)</span>-test for a population proportion requires some conditions on the sample size in order for the approximate sampling distribution of the test statistic to be appropriate. These depend also on the value of <span class="math inline">\(\pi\)</span>, which we can estimate by <span class="math inline">\(\hat{\pi}\)</span>. One rule of thumb is that <span class="math inline">\(n\)</span> should be larger than 10 divided by <span class="math inline">\(\pi\)</span> or <span class="math inline">\(1-\pi\)</span>, whichever is smaller. When <span class="math inline">\(\pi\)</span> is not very small or very large, e.g. if it is between 0.3 and 0.7, this essentially amounts to the condition that <span class="math inline">\(n\)</span> should be at least 30. In the voting example 5.1, where <span class="math inline">\(\hat{\pi}=0.47\)</span>, the sample size of <span class="math inline">\(n=702\)</span> is clearly large enough. In the jury example 5.2, <span class="math inline">\(\hat{\pi}=0.0457\)</span> is much closer to zero, but since <span class="math inline">\(10/0.0457\)</span> is a little over 200, a sample of <span class="math inline">\(n=4950\)</span> is again sufficient.</p>
<p>When the sample size is large enough, the sampling distribution of <span class="math inline">\(z\)</span> defined by (\ref{eq:ztestp}) is approximately the <strong>standard normal distribution</strong>. The probability curve of this distribution is shown in Figure <a href="c-probs.html#fig:f-pval-prob">5.1</a>. For now we just take it as given, and postpone a general discussion of the normal distribution until Chapter <a href="#c-means"><strong>??</strong></a>.</p>
<div class="figure"><span id="fig:f-pval-prob"></span>
<img src="pval_zp.png" alt=" Illustration of the calculation of P-values from the standard normal distribution. Here the value of the z-test statistic is z=-1.59 (as in the referendum example 5.1). The areas in grey indicate the two-sided P-values, i.e. the probabilities of values at least as far from 0 as the observed value of z." width="377" />
<p class="caption">Figure 5.1:  Illustration of the calculation of <span class="math inline">\(P\)</span>-values from the standard normal distribution. Here the value of the <span class="math inline">\(z\)</span>-test statistic is <span class="math inline">\(z=-1.59\)</span> (as in the referendum example 5.1). The areas in grey indicate the two-sided <span class="math inline">\(P\)</span>-values, i.e. the probabilities of values at least as far from 0 as the observed value of <span class="math inline">\(z\)</span>.</p>
</div>
<p>The <span class="math inline">\(P\)</span>-value of the test is calculated from this distribution using the general principles introduced in Section <a href="c-tables.html#ss-tables-chi2test-Pval">4.3.5</a>. In other words, the <span class="math inline">\(P\)</span>-value is the probability that the test statistic <span class="math inline">\(z\)</span> has a value that is as or more extreme than the value of <span class="math inline">\(z\)</span> in the observed sample. Now, however, the details of this calculation depend also on the alternative hypothesis, so some additional explanation is needed.</p>
<p>Consider first the more common case of a two-sided alternative hypothesis (\ref{eq:Hatwo}), that <span class="math inline">\(\Delta\ne 0\)</span>. As discussed in the previous section, it is <em>large</em> values of the test statistic which indicate evidence against the null hypothesis, because a large <span class="math inline">\(z\)</span> is obtained when the sample difference <span class="math inline">\(\hat{\Delta}=\hat{\pi}-\pi_{0}\)</span> is very different from the zero difference claimed by the null hypothesis. When the alternative is two-sided, “large” is taken to mean any value of <span class="math inline">\(z\)</span> far from zero, i.e. either large positive or large negative values, because both indicate that the sample difference is far from 0. If <span class="math inline">\(z\)</span> is large and positive, <span class="math inline">\(\hat{\Delta}\)</span> is much <em>larger</em> than 0. In example 5.1 this would indicate that a much larger proportion than 0.5 of the sample say they intend to vote Yes. If <span class="math inline">\(z\)</span> is large and negative, <span class="math inline">\(\hat{\Delta}\)</span> is much <em>smaller</em> than 0, indicating a much smaller sample proportion than 0.5. Both of these cases would count as evidence against <span class="math inline">\(H_{0}\)</span> when the alternative hypothesis is two-sided.</p>
<p>The observed value of the <span class="math inline">\(z\)</span>-test statistic in Example 5.1 was actually <span class="math inline">\(z=-1.59\)</span>. Evidence would thus be “as strong” against <span class="math inline">\(H_{0}\)</span> as the observed <span class="math inline">\(z\)</span> if we obtained a <span class="math inline">\(z\)</span>-test statistic of <span class="math inline">\(-1.59\)</span> or 1.59, the value exactly as far from 0 as the observed <span class="math inline">\(z\)</span> but above rather than below 0. Similarly, evidence against the null would be even stronger if <span class="math inline">\(z\)</span> was further from zero than 1.59, i.e. larger than 1.59 or smaller than <span class="math inline">\(-1.59\)</span>. To obtain the <span class="math inline">\(P\)</span>-value, we thus need to calculate the probability of observing a <span class="math inline">\(z\)</span>-test statistic which is at most <span class="math inline">\(-1.59\)</span> or at least 1.59 when the null hypothesis is true in the population. In general, the <span class="math inline">\(P\)</span>-value for testing the null hypothesis against a two-sided alternative is the probability of obtaining a value at least <span class="math inline">\(z\)</span> or at most <span class="math inline">\(-z\)</span> (when <span class="math inline">\(z\)</span> is positive, vice versa when it is negative), where <span class="math inline">\(z\)</span> here denotes the value of the test statistic in the sample. Such probabilities are calculated from the approximately standard normal sampling distribution of the test statistic under <span class="math inline">\(H_{0}\)</span>.</p>
<p>This calculation of the <span class="math inline">\(P\)</span>-value is illustrated graphically in Figure <a href="c-probs.html#fig:f-pval-prob">5.1</a>. The curve in the plot is that of the standard normal distribution. Two areas are shown in grey under the curve, one on each tail of the distribution. The one on the left corresponds to values of <span class="math inline">\(-1.59\)</span> and smaller, and the one on the right to values of 1.59 or larger. Each of these areas is about 0.056, and the <span class="math inline">\(P\)</span>-value for a test against a two-sided alternative is their combined area, i.e. <span class="math inline">\(P=0.056+0.056=0.112\)</span>. This means that even if the true population proportion of Yes-voters was actually exactly 0.5, there would be a probability of 0.112 of obtaining a test statistic as or more extreme than the <span class="math inline">\(z=-1.59\)</span> that was actually observed in Example 5.1.</p>
<p>In example 5.2 the observed test statistic was <span class="math inline">\(z=-16.71\)</span>. The two-sided <span class="math inline">\(P\)</span>-value is then the probability of values that are at most <span class="math inline">\(-16.71\)</span> or at least 16.71. These areas are not shown in Figure <a href="c-probs.html#fig:f-pval-prob">5.1</a> because they would not be visible in it. The horizontal axis of the figures runs from <span class="math inline">\(-4\)</span> to <span class="math inline">\(+4\)</span>, so <span class="math inline">\(-16.71\)</span> is clearly far in the tail of the distribution and the corresponding probability is very small; we would report it as <span class="math inline">\(P&lt;0.001\)</span>.</p>
<p>Consider now the case of a one-sided alternative hypothesis. For example, in the referendum example we might have decided beforehand to focus only on the possiblity that the proportion of people who intend to vote Yes is smaller than 0.5, and hence consider the alternative hypothesis that <span class="math inline">\(\Delta&lt;0\)</span>. Two situations might then arise. First, suppose that the observed value of the sample difference is in the direction indicated by the alternative hypothesis. This is the case in the example, where the sample difference <span class="math inline">\(\hat{\Delta}=-0.03\)</span> is indeed smaller than zero, and the test statistic <span class="math inline">\(t=-1.59\)</span> is negative. The possible values of <span class="math inline">\(z\)</span> contributing to the <span class="math inline">\(P\)</span>-value are then those of <span class="math inline">\(-1.59\)</span> or smaller. Values of <span class="math inline">\(1.59\)</span> and larger are now not included, because positive values of the test statistic (corresponding to sample differences greater than 0) would not be regarded as evidence in favour of the claim that <span class="math inline">\(\Delta\)</span> is smaller than 0. The <span class="math inline">\(P\)</span>-value is thus only the probability corresponding to the area on the left tail of the curve in Figure <a href="c-probs.html#fig:f-pval-prob">5.1</a>, and the corresponding area on the right tail is not included. Since both areas have the same size, the one-sided <span class="math inline">\(P\)</span>-value is half the two-sided value, i.e. 0.056 instead of 0.112. In general, the one-sided <span class="math inline">\(P\)</span>-value for a <span class="math inline">\(z\)</span>-test of a proportion and other similar tests is always obtained by dividing the two-sided value by 2, if the sample evidence is in the direction of the one-sided alternative hypothesis.</p>
<p>The second case occurs when the sample difference is not in the direction indicated by a one-sided alternative hypothesis. For example, suppose that the sample proportion of Yes-voters had actually been 0.53, i.e. 0.03 larger than 0.5, so that we had obtained <span class="math inline">\(z=+1.59\)</span> instead. The possible values of the test statistic which contributed to the <span class="math inline">\(P\)</span>-value would then be <span class="math inline">\(z=1.59\)</span> and all smaller values. These are “as strong or stronger evidence against the null hypothesis and in the direction of the alternative hypothesis” as required by the definition at the beginning of Section <a href="c-tables.html#ss-tables-chi2test-Pval">4.3.5</a>, since they agree with the alternative hypothesis (negative values of <span class="math inline">\(z\)</span>) or at least disagree with it less than the observed <span class="math inline">\(z\)</span> (positive values from 0 to 1.59). In Figure <a href="c-probs.html#fig:f-pval-prob">5.1</a>, these values would correspond to the area under the whole curve, apart from the region to the right of <span class="math inline">\(1.59\)</span> on the right tail. Since the probability of the latter is 0.056 and the total probability under the curve is 1, the required probability is <span class="math inline">\(P=1-0.0.56=0.944\)</span>. However, calculating the <span class="math inline">\(P\)</span>-value so precisely is hardly necessary in this case, as it is clearly going to be closer to 1 than to 0. The conclusion from such a large <span class="math inline">\(P\)</span>-value will always be that the null hypothesis should not be rejected. This is also intuitively obvious, as a sample difference in the opposite direction from the one claimed by the alternative hypothesis is clearly not to be regarded as evidence in favour of that alternative hypothesis. In short, if the sample difference is in a different direction than a one-sided alternative hypothesis, the <span class="math inline">\(P\)</span>-value can be reported simply as <span class="math inline">\(P&gt;0.5\)</span> without further calculations.</p>
<p>If a statistical software package is used to carry out the test, it will also report the <span class="math inline">\(P\)</span>-value and no further calculations are needed (except dividing a two-sided <span class="math inline">\(P\)</span>-value by 2, if a one-sided value is needed and only a two-sided one is reported). However, since SPSS does not currently provide a procedure for this test, and for exam purposes, we will briefly outline how an approximate <span class="math inline">\(P\)</span>-value is obtained using critical values from a table. This is done in a very similar way as for the <span class="math inline">\(\chi^{2}\)</span> test in Section <a href="c-tables.html#ss-tables-chi2test-Pval">4.3.5</a>.</p>
<p>The first part of Table <a href="c-probs.html#tab:t-ttable">5.2</a> shows a table of critical values for the standard normal distribution. These values are also shown in Section <a href="appendix.html#s-disttables-t">7.2.2</a> at the end of this course pack, on the last row of a larger table (the other parts of this table will be explained later, in Section <a href="#ss-means-inference-variants"><strong>??</strong></a>). A version of this table is included in all introductory text books on statistics, although its format may be slightly different in different books.</p>
<table>
<caption><span id="tab:t-ttable">Table 5.2: </span> A table of critical values for the standard normal distribution. The upper part of the table shows the critical values in one row, as in standard statistical tables (see the last row of the table in Section <a href="appendix.html#s-disttables-t">7.2.2</a>). The lower part of the table includes the same numbers rearranged to show critical values for conventional significance levels for one- and two-sided tests.</caption>
<tbody>
<tr class="odd">
<td></td>
<td align="right">0.100</td>
<td align="right">0.050</td>
<td align="right">0.025</td>
<td align="right">0.01</td>
<td align="right">0.005</td>
<td align="right">0.001</td>
<td align="right">0.0005</td>
</tr>
<tr class="even">
<td>Critical value</td>
<td align="right">1.282</td>
<td align="right">1.645</td>
<td align="right">1.960</td>
<td align="right">2.326</td>
<td align="right">2.576</td>
<td align="right">3.090</td>
<td align="right">3.291</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="right">Significance levels</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Alternative hypothesis</td>
<td align="right">0.10</td>
<td>0.05</td>
<td>0.01</td>
<td>0.001</td>
</tr>
<tr class="odd">
<td>Two-sided</td>
<td align="right">1.65</td>
<td>1.96</td>
<td>2.58</td>
<td>3.29</td>
</tr>
<tr class="even">
<td>One-sided</td>
<td align="right">1.28</td>
<td>1.65</td>
<td>2.33</td>
<td>3.09</td>
</tr>
</tbody>
</table>
<p>The columns of the first part of Table <a href="c-probs.html#tab:t-ttable">5.2</a> are labelled “Right-hand tail probabilities”, with separate columns for some values from 0.100 to 0.0005. This means that the probability that a value from the standard normal distribution is at least as large as the value given in a particular column is the number given at the top of that column. For example, the value in the column labelled “0.025” is 1.960, indicating that the probability of obtaining a value equal to or greater than 1.960 from the standard normal distribution is 0.025. Because the distribution is symmetric, the probability of values of at most <span class="math inline">\(-1.960\)</span> is also 0.025, and the total probability that a value is at least 1.960 units from zero is <span class="math inline">\(0.025+0.025=0.05\)</span>.</p>
<p>These values can be used to obtain bounds for <span class="math inline">\(P\)</span>-values, expressed in terms of conventional significance levels of 0.10, 0.05, 0.01 and 0.001. The values at which these tail probabilities are obtained are the corresponding critical values for the test statistic. They are shown in the lower part of Table <a href="c-probs.html#tab:t-ttable">5.2</a>, slightly rearranged for clarity of presentation and rounded to two decimal places (which is accurate enough for practical purposes). The basic idea of using the critical values is that if the observed (absolute value of) the <span class="math inline">\(z\)</span>-test statistic is <em>larger</em> than a critical value (for the required kind of alternative hypothesis) shown in the lower part of Table <a href="c-probs.html#tab:t-ttable">5.2</a>, the <span class="math inline">\(P\)</span>-value is <em>smaller</em> than the significance level corresponding to that critical value.</p>
<p>The table shows only positive critical values. If the observed test statistic is actually negative, its negative (<span class="math inline">\(-\)</span>) sign is omitted and the resulting positive value (i.e. the absolute value of the statistic) is compared to the critical values. Note also that the critical value for a given significance level depends on whether the alternative hypothesis is two-sided or one-sided. In the one-sided case, the test statistic is compared to the critical values only if it is actually in the direction of the alternative hypothesis; if not, we can simply report <span class="math inline">\(P&gt;0.5\)</span> as discussed above.</p>
<p>The <span class="math inline">\(P\)</span>-value obtained from the table is reported as being smaller than the smallest conventional significance level for which the corresponding critical value is exceeded by the observed test statistic. For instance, in the jury example 5.2 we have <span class="math inline">\(z=-16.71\)</span>. Considering a two-sided alternative hypothesis, 16.71 is larger than the critical values 1.65, 1.96, 2.58 and 3.29 for all the standard significance levels, so we can report that <span class="math inline">\(P&lt;0.001\)</span>. For Example 5.1, in contrast, <span class="math inline">\(z=-1.59\)</span>, the absolute value of which is smaller than even the critical value 1.65 for the 10% significance level. For this example, we would report <span class="math inline">\(P&gt;0.1\)</span>.</p>
<p>The intuitive idea of the critical values and their connection to the <span class="math inline">\(P\)</span>-values is illustrated for Example 5.1 by Figure <a href="c-probs.html#fig:f-pval-prob">5.1</a>. Here the observed test statistic is <span class="math inline">\(t=-1.59\)</span>, so the two-sided <span class="math inline">\(P\)</span>-value is the probability of values at least 1.59 or at most <span class="math inline">\(-1.59\)</span>, which correspond to the two gray areas in the tails of the distribution. Also shown in the plot is one of the critical values for two-sided tests, the 1.96 for significance level 0.05. By definition of the critical values, the combined tail probability of values at least 1.96 from 0, i.e. the probability of values at least 1.96 or at most <span class="math inline">\(-1.96\)</span>, is 0.05. It is clear from the plot that since 1.59 is smaller than 1.96, these areas are smaller than the tail areas corresponding to 1.59 and <span class="math inline">\(-1.59\)</span>, and the combined area of the latter must be more than 0.05, i.e. it must be that <span class="math inline">\(P&gt;0.05\)</span>. Similar argument for the 10% critical value of 1.65 shows that <span class="math inline">\(P\)</span> is here also larger than 0.1.</p>
</div>
<div id="ss-probs-test1sample-conclusions" class="section level3">
<h3><span class="header-section-number">5.5.4</span> Conclusions from the test</h3>
<p>The general principles of drawing and stating conclusions from a significance test have already been explained in Section <a href="c-tables.html#ss-tables-chi2test-conclusions">4.3.6</a>, so they need not be repeated here. Considering two-sided alternative hypotheses, the conclusions in our two examples are as follows:</p>
<ul>
<li><p>In the referendum example 5.1, <span class="math inline">\(P=0.112\)</span> for the null hypothesis that <span class="math inline">\(\pi=0.5\)</span> in the population of eligible voters. The null hypothesis is not rejected at conventional levels of significance. There is not enough evidence to conclude that the proportion of voters who definitely intend to vote Yes differs from one half.</p></li>
<li><p>In the jury example 5.2, <span class="math inline">\(P&lt;0.001\)</span> for the null hypothesis that <span class="math inline">\(\pi=0.124\)</span>. The null hypothesis is thus overwhelmingly rejected at any conventional level of significance. There is very strong evidence that the probability of a black person being selected to the jury pool differs from the proportion of blacks in the population of the county.</p></li>
</ul>
</div>
<div id="ss-probs-test1sample-summary" class="section level3">
<h3><span class="header-section-number">5.5.5</span> Summary of the test</h3>
<p>As a summary, let us again repeat the main steps of the test described in this section in a concise form, using the voting variable of Example 5.1 for illustration:</p>
<ol style="list-style-type: decimal">
<li><p>Data: a sample of size <span class="math inline">\(n=702\)</span> of a dichotomous variable <span class="math inline">\(Y\)</span> with values 1 (Yes) and 0 (No or undecided), with the sample proportion of ones <span class="math inline">\(\hat{\pi}=0.47\)</span>.</p></li>
<li><p>Assumptions: the observations are a random sample from a population distribution with some population proportion (probability) <span class="math inline">\(\pi\)</span>, and the sample size <span class="math inline">\(n\)</span> is large enough for the test to be valid (for example, <span class="math inline">\(n\ge 30\)</span> when <span class="math inline">\(\pi_{0}\)</span> is between about 0.3 and 0.7, as it is here).</p></li>
<li><p>Hypotheses: null hypothesis <span class="math inline">\(H_{0}: \pi=\pi_{0}\)</span> against the alternative hypothesis <span class="math inline">\(H_{a}: \pi\ne \pi_{0}\)</span>, where <span class="math inline">\(\pi_{0}=0.5\)</span>.</p></li>
<li><p>The test statistic: the <span class="math inline">\(z\)</span>-statistic <span class="math display">\[z=\frac{\hat{\pi}-\pi_{0}}{\sqrt{\pi_{0}(1-\pi_{0})/n}}=
\frac{0.47-0.50}{\sqrt{0.50\times(1-0.50)/702}}=-1.59.\]</span></p></li>
<li><p>The sampling distribution of the test statistic when <span class="math inline">\(H_{0}\)</span> is true: a standard normal distribution.</p></li>
<li><p>The <span class="math inline">\(P\)</span>-value: the probability that a randomly selected value from the the standard normal distribution is at most <span class="math inline">\(-1.59\)</span> or at least 1.59, which is <span class="math inline">\(P=0.112\)</span>.</p>
<ul>
<li>If the precise <span class="math inline">\(P\)</span>-value is not available, we can observe that 1.59 is smaller than the two-sided critical value 1.65 for the 10% level of significance. Thus it must be that <span class="math inline">\(P&gt;0.1\)</span>.</li>
</ul></li>
<li><p>Conclusion: The null hypothesis is not rejected (<span class="math inline">\(P=0.112\)</span>). There is not enough evidence to conclude that the proportion of eligible voters who definitely intend to vote Yes differs from one half. Based on this opinion poll, the referendum remains too close to call.</p></li>
</ol>
</div>
</div>
<div id="s-probs-1sampleci" class="section level2">
<h2><span class="header-section-number">5.6</span> Confidence interval for a single proportion</h2>
<div id="s-probs-1sampleci-intro" class="section level3">
<h3><span class="header-section-number">5.6.1</span> Introduction</h3>
<p>A significance test assesses whether it is plausible, given the evidence in the observed data, that a population parameter or parameters have a specific set of values claimed by the null hypothesis. For example, in Section <a href="c-probs.html#s-probs-test1sample">5.5</a> we asked such a question about the probability parameter of a binary variable in a single population.</p>
<p>In many ways a more natural approach would be try to identify all of those values of a parameter which <em>are</em> plausible given the data. This leads to a form of statistical inference known as <strong>interval estimation</strong>, which aims to present not only a single best guess (i.e. a point estimate) of a population parameter, but also a range of plausible values (an <strong>interval estimate</strong>) for it. Such an interval is known as a <strong>confidence interval</strong>. This section introduces the idea of confidence intervals, and shows how to construct them for a population probability. In later sections, the same principles will be used to calculate confidence intervals for other kinds of population parameters.</p>
<p>Interval estimation is an often underused part of statistical inference, while significance testing is arguably overused or at least often misused. In most contexts it would be useful to report confidence intervals in addition to, or instead of, results of significance tests. This is not done often enough in research publications in the social sciences.</p>
</div>
<div id="s-probs-1sampleci-calc" class="section level3">
<h3><span class="header-section-number">5.6.2</span> Calculation of the interval</h3>
<p>Our aim is again to draw inference on the difference <span class="math inline">\(\Delta=\pi-\pi_{0}\)</span> or, equivalently, the population probability <span class="math inline">\(\pi\)</span>. The point estimate of <span class="math inline">\(\Delta\)</span> is <span class="math inline">\(\hat{\Delta}=\hat{\pi}-\pi_{0}\)</span> where <span class="math inline">\(\hat{\pi}\)</span> is the sample proportion corresponding to <span class="math inline">\(\pi\)</span>. Suppose that the conditions on the sample size <span class="math inline">\(n\)</span> that were discussed in Section <a href="c-probs.html#ss-probs-test1sample-samplingd">5.5.3</a> are again satisfied.</p>
<p>Consider now Figure @ref(fig:f-pval-prob}. One of the results illustrated by it is that if <span class="math inline">\(\pi_{0}\)</span> is the true value of of the population probability <span class="math inline">\(\pi\)</span>, so that <span class="math inline">\(\Delta=\pi-\pi_{0}=0\)</span>, there is a probability of 0.95 that for a randomly drawn sample from the population the <span class="math inline">\(z\)</span>-test statistic <span class="math inline">\(z=\hat{\Delta}/\hat{\sigma}_{\hat{\Delta}}\)</span> is between <span class="math inline">\(-1.96\)</span> and <span class="math inline">\(+1.96\)</span>. This also implies that the probability is 0.95 that in such a sample the observed value of <span class="math inline">\(\hat{\Delta}\)</span> will be between <span class="math inline">\(\Delta-1.96\,\hat{\sigma}_{\hat{\Delta}}\)</span> and <span class="math inline">\(\Delta+1.96\,\hat{\sigma}_{\hat{\Delta}}\)</span>. Furthermore, it is clear from the figure that all of the values within this interval are more likely to occur than any of the values outside the interval (i.e. those in the two tails of the sampling distribution). The interval thus seems like a sensible summary of the “most likely” values that the estimate <span class="math inline">\(\hat{\Delta}\)</span> may have in random samples.</p>
A confidence interval essentially turns this around, into a statement about the unknown true value of <span class="math inline">\(\Delta\)</span> in the population, even in cases where <span class="math inline">\(\Delta\)</span> is not 0. This is done by substituting <span class="math inline">\(\hat{\Delta}\)</span> for <span class="math inline">\(\Delta\)</span> above, to create the interval
<span class="math display">\[\begin{equation}\text{from  }
\hat{\Delta} -1.96\times \hat{\sigma}_{\hat{\Delta}}
\text{  to  }
\hat{\Delta}
+1.96\times \hat{\sigma}_{\hat{\Delta}}.
\label{eq:cim0}\end{equation}\]</span>
This is the <strong>95 % confidence interval</strong> for the population difference <span class="math inline">\(\Delta\)</span>. It is usually written more concisely as
<span class="math display">\[\begin{equation}\hat{\Delta}
\pm 1.96\, \hat{\sigma}_{\hat{\Delta}}
\label{eq:cim1}\end{equation}\]</span>
<p>where the “plusminus” symbol <span class="math inline">\(\pm\)</span> indicates that we calculate the two endpoints of the interval as in (\ref{eq:cim0}), one below and one above <span class="math inline">\(\hat{\Delta}\)</span>.</p>
Expression (\ref{eq:cim1}) is general in the sense that many different quantities can take the role of <span class="math inline">\(\Delta\)</span> in it. Here we consider for now the case of <span class="math inline">\(\Delta=\pi-\pi_{0}\)</span>. The estimated standard error <span class="math inline">\(\hat{\sigma}_{\hat{\Delta}}\)</span> is analogous to (\ref{eq:seDhatp}) used for the <span class="math inline">\(z\)</span>-test, but not the same. This is because the confidence interval is not calculated under the null hypothesis <span class="math inline">\(H_{0}:\; \pi=\pi_{0}\)</span>, so we cannot use <span class="math inline">\(\pi_{0}\)</span> for <span class="math inline">\(\pi\)</span> in the standard error. Instead, <span class="math inline">\(\pi\)</span> is estimated by the sample proportion <span class="math inline">\(\hat{\pi}\)</span>, giving the estimated standard error
<span class="math display">\[\begin{equation}\hat{\sigma}_{\hat{\Delta}} = \sqrt{
\frac{\hat{\pi}(1-\hat{\pi})}{n}
}
\label{eq:sephat2}\end{equation}\]</span>
and thus the 95% confidence interval <span class="math display">\[(\hat{\pi}-\pi_{0}) \pm 1.96 \;
\sqrt{
\frac{\hat{\pi}(1-\hat{\pi})}{n}}\]</span> for <span class="math inline">\(\Delta=\pi-\pi_{0}\)</span>. Alternatively, a confidence interval for <span class="math inline">\(\pi\)</span> itself is given by
<span class="math display">\[\begin{equation}\hat{\pi} \pm 1.96 \;
\sqrt{
\frac{\hat{\pi}(1-\hat{\pi})}{n}}.
\label{eq:cip2}\end{equation}\]</span>
<p>This is typically the most useful interval for use in practice. For instance, in the referendum example 5.1 this gives a 95% confidence interval of <span class="math display">\[0.470\pm 1.96\times \sqrt{\frac{0.470\times(1-0.470)}{702}}
=0.470\pm 0.0369=(0.433, 0.507)\]</span> for the proportion of definite Yes-voters in the population. Similarly, in Example 5.2 the 95% confidence interval for the probability of a black person being selected for the jury pool is (0.040, 0.052). These intervals are also shown in Table <a href="#tab:t-probex">5.1</a>.</p>
</div>
<div id="s-probs-1sampleci-int" class="section level3">
<h3><span class="header-section-number">5.6.3</span> Interpretation of confidence intervals</h3>
<p>As with the <span class="math inline">\(P\)</span>-value of a significance test, the precise interpretation of a confidence interval refers to probabilities calculated from a sampling distribution, i.e. probabilities evaluated from a hypothetical exercise of repeated sampling:</p>
<ul>
<li>If we obtained many samples from the population and calculated the confidence interval for each such sample using the formula (\ref{eq:cip2}), approximately 95% of these intervals would contain the true value of the population proportion <span class="math inline">\(\pi\)</span>.</li>
</ul>
<p>This is undeniably convoluted, even more so than the precise interpretation of a <span class="math inline">\(P\)</span>-value. In practise a confidence interval would not usually be described in exactly these words. Instead, a research report might, for example, write that (in the referendum example) “the 95 % confidence interval for the proportion of eligible voters in the population who definitely intend to vote Yes is (0.433, 0.507)”, or that “we are 95 % confident that the proportion of eligible voters in the population who definitely intend to vote Yes is between 43.3% and 50.7%”. Such a statement in effect assumes that the readers will be familiar enough with the idea of confidence intervals to understand the claim. It is nevertheless useful to be aware of the more careful interpretation of a confidence interval, if only to avoid misunderstandings. The most common error is to claim that “there is a 95% probability that the proportion in the population is between 0.433 and 0.507”. Although the difference to the interpretation given above may seem small, the latter statement is not really true, or strictly speaking even meaningful, in the statistical framework considered here.</p>
In place of the 1.96 in (\ref{eq:cim1}), we may also use other numbers. To allow for this in the notation, we can also write
<span class="math display">\[\begin{equation}\hat{\Delta} \pm z_{\alpha/2}\; \hat{\sigma}_{\hat{\Delta}}.
\label{eq:ci-D-gen}\end{equation}\]</span>
<p>where the multiplier <span class="math inline">\(z_{\alpha/2}\)</span> is a number which depends on two things. One of them is the sampling distribution of <span class="math inline">\(\hat{\Delta}\)</span>, which is here assumed to be the normal distribution (another possibility is discussed in Section <a href="#ss-means-inference-variants"><strong>??</strong></a>). The second is the <strong>confidence level</strong> which we have chosen for the confidence interval. For example, the probability of 0.95 in the interpretation of a 95% confidence interval discussed above is the confidence level of that interval. Conventionally the 0.95 level is most commonly used, while other standard choices are 0.90 and 0.99, i.e. 90% and 99% confidence intervals.</p>
<p>In the symbol <span class="math inline">\(z_{\alpha/2}\)</span>, <span class="math inline">\(\alpha\)</span> is a number such that <span class="math inline">\(1-\alpha\)</span> equals the required confidence level. In other words, <span class="math inline">\(\alpha=0.1\)</span>, 0.05, and 0.01 for confidence levels of <span class="math inline">\(1-\alpha=0.90\)</span>, 0.95 and 0.99 respectively. The values that are required for the conventional levels are <span class="math inline">\(z_{0.10/2}=z_{0.05}=1.64\)</span>, <span class="math inline">\(z_{0.05/2}=z_{0.025}=1.96\)</span>, and <span class="math inline">\(z_{0.01/2}=z_{0.005}=2.58\)</span>, which correspond to intervals at the confidence levels of 90%, 95% and 99% respectively. These values are also shown in Table <a href="c-probs.html#tab:t-ciq">5.3</a>.</p>
<table>
<caption><span id="tab:t-ciq">Table 5.3: </span> Multipliers <span class="math inline">\(z_{\alpha/2}\)</span> used to obtain confidence intervals based on the normal distribution, for three standard confidence levels. These values are substituted for <span class="math inline">\(z_{\alpha/2}\)</span> in formula (\ref{eq:ci-D-gen}) to obtain the confidence interval.</caption>
<tbody>
<tr class="odd">
<td></td>
<td align="right">Confidence levels</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td></td>
<td align="right">90%</td>
<td align="right">95%</td>
<td align="right">99%</td>
</tr>
<tr class="odd">
<td>Multiplier <span class="math inline">\(z_{\alpha/2}\)</span></td>
<td align="right">1.64</td>
<td align="right">1.96</td>
<td align="right">2.58</td>
</tr>
</tbody>
</table>
<p>A confidence interval contains, loosely speaking, those numbers which are considered plausible values for the unknown population difference <span class="math inline">\(\Delta\)</span> in the light of the evidence in the data. The <em>width</em> of the interval thus reflects our uncertainty about the exact value of <span class="math inline">\(\Delta\)</span>, which in turn is related to the amount of information the data provide about <span class="math inline">\(\Delta\)</span>. If the interval is wide, many values are consistent with the observed data, so there is still a large amount of uncertainty; if the interval is narrow, we have much information about <span class="math inline">\(\Delta\)</span> and thus little uncertainty. Another way of stating this is that when the confidence interval is narrow, estimates of <span class="math inline">\(\Delta\)</span> are very <em>precise</em>.</p>
<p>The width of the interval (\ref{eq:ci-D-gen}) is <span class="math inline">\(2\times z_{\alpha/2}\times \hat{\sigma}_{\hat{\Delta}}\)</span>. This depends on</p>
<ul>
<li><p>The confidence level: the higher the level, the wider the interval. Thus a 99% confidence interval is always wider than a 95% interval for the same data, and wider still than a 90% interval. This is logically inevitable: if we want to state with high level of confidence that a parameter is within a certain interval, we must allow the interval to contain a wide range of values. It also explains why we do not consider a 100% confidence interval: this would contain all possible values of <span class="math inline">\(\Delta\)</span> and exclude none, making no use of the data at all. Instead, we aim for a high but not perfect level of confidence, obtaining an interval which contains some but not all possible values, for the price of a small chance of incorrect conclusions.</p></li>
<li><p>The standard error <span class="math inline">\(\hat{\sigma}_{\hat{\Delta}}\)</span>, which in the case of a single proportion is (\ref{eq:sephat2}). This in turn depends on</p>
<ul>
<li><p>the sample size <span class="math inline">\(n\)</span>: the larger this is, the narrower the interval. Increasing the sample size thus results (other things being equal) in reduced uncertainty and higher precision.</p></li>
<li><p>the true population proportion <span class="math inline">\(\pi\)</span>: the closer this is to 0.5, the wider the interval. Unlike the sample size, this determinant of the estimation uncertainty is not in our control.</p></li>
</ul></li>
</ul>
<p>Opinion polls of the kind illustrated by the referendum example are probably where non-academic audiences are most likely to encounter confidence intervals, although not under that label. Media reports of such polls typically include a <em>margin of error</em> for the results. For example, in the referendum example it might be reported that 47% of the respondents said that they would definitely vote Yes, and that “the study has a margin of error of plus or minus four percentage points”. In most cases the phrase “margin of error” refers to a 95% confidence interval. Unless otherwise mentioned, we can thus take a statement like the one above to mean that the 95% confidence interval for the proportion of interest is approximately <span class="math inline">\(47\pm 4\)</span> percentage points. For a realistic interpretation of the implications of the results, the width of this interval is at least as important as the point estimate of the proportion. This is often neglected in media reports of opinion polls, where the point estimate tends to be headline news, while the margin of error is typically mentioned only in passing or omitted altogether.</p>
</div>
<div id="ss-means-ci-vstests" class="section level3">
<h3><span class="header-section-number">5.6.4</span> Confidence intervals vs. significance tests</h3>
<p>There are some obvious similarities between the conclusions from significance tests and confidence intervals. For example, a <span class="math inline">\(z\)</span>-test in the referendum example 5.1 showed that the null hypothesis that the population proportion <span class="math inline">\(\pi\)</span> was 0.5 was not rejected (<span class="math inline">\(P=0.112\)</span>). Thus 0.5 is a plausible value for <span class="math inline">\(\pi\)</span> in light of the observed data. The 95% confidence interval for <span class="math inline">\(\pi\)</span> showed that, at this level of confidence, plausible values for <span class="math inline">\(\pi\)</span> are those between 0.433 and 0.507. In particular, these include 0.5, so the confidence interval also indicates that a proportion of 0.5 is plausible. This connection between the test and the confidence interval is in fact exact:</p>
<ul>
<li>If the hypothesis <span class="math inline">\(H_{0}: \Delta=0\)</span> about a population quantity <span class="math inline">\(\Delta\)</span> is rejected at the 5% level of significance using the <span class="math inline">\(z\)</span>-test against a <em>two-sided</em> alternative hypothesis, the 95 % confidence interval for <span class="math inline">\(\Delta\)</span> will not contain 0, and vice versa. Similarly, if <span class="math inline">\(H_{0}\)</span> is not rejected, the confidence interval will contain 0, and vice versa.</li>
</ul>
<p>The same is true for other matching pairs of levels of significance and confidence, e.g. for a test with a 1% level of significance and a 99% (i.e. (100-1)%) confidence interval. In short, the significance test and the confidence interval will in these cases always give the same answer about whether or not a parameter value is plausible (consistent with the data) at a given level of significance/confidence.</p>
<p>These pairs of a test and an interval are exactly comparable in that they concern the same population parameter, estimate all parameters in the same way, use the same sampling distribution for inference, and use the same level of significance/confidence. Not all tests and confidence intervals have exact pairs in this way. Also, some tests are for hypotheses about more than one parameter at once, so there is no corresponding single confidence interval. Nevertheless, the connection stated above is useful for understanding the ideas of both tests and confidence intervals.</p>
<p>These results also illustrate how confidence intervals are inherently more informative than significance tests. For instance, in the jury example 5.2, both the test and the confidence interval agree on the implausibility of the claim that the population probability of being selected to the jury panel is the same as the proportion (0.124) of black people in the population, since the claim that <span class="math inline">\(\pi=0.124\)</span> is rejected by the test (with <span class="math inline">\(P&lt;0.001\)</span>) and outside the interval <span class="math inline">\((0.040; 0.052)\)</span>. Unlike the test, however, the confidence interval summarizes the plausibility of <em>all</em> possible values of <span class="math inline">\(\pi\)</span> and not just <span class="math inline">\(\pi_{0}=0.124\)</span>. One way to describe this is to consider what would have happened if we had carried out a series of significance tests of null hypotheses of the form <span class="math inline">\(H_{0}: \pi=\pi_{0}\)</span> for a range of values of <span class="math inline">\(\pi_{0}\)</span>. The confidence interval contains all those values <span class="math inline">\(\pi_{0}\)</span> which would not have been rejected by the test, while all the values outside the interval would have been rejected. Here <span class="math inline">\(H_{0}: \pi=\pi_{0}\)</span> would thus not have been rejected at the 5% level if <span class="math inline">\(\pi_{0}\)</span> had been between 0.040 and 0.052, and rejected otherwise. This, of course, is not how significance tests are actually conducted, but it provides a useful additional interpretation of confidence intervals.</p>
<p>A confidence interval is particularly useful when the parameter of interest is measured in familiar units, such as the proportions considered so far. We may then try to judge, in substantive terms, how wide the interval is and how far it is from particular values of interest. In the jury example the 95% confidence interval ranges from 4.0% to 5.2%, which suggests that the population probability is estimated fairly precisely by this survey. The interval also reveals that even its upper bound is less than half of the figure of 12.4% which would correspond to proportional representation of blacks in the jury pool, a result which suggests quite substantial underrepresentation in the pool.</p>
</div>
</div>
<div id="s-probs-2samples" class="section level2">
<h2><span class="header-section-number">5.7</span> Inference for comparing two proportions</h2>
<p>In Examples 5.3 and 5.4, the aim is to compare the proportion of a dichotomous response variable <span class="math inline">\(Y\)</span> between two groups of a dichotomous explanatory variable <span class="math inline">\(X\)</span>:</p>
<ul>
<li><p>Example 5.3: compare the proportion of polio cases among the unvaccinated (<span class="math inline">\(\pi_{1}\)</span>) and vaccinated (<span class="math inline">\(\pi_{2}\)</span>) children.</p></li>
<li><p>Example 5.4: compare the proportion of optimistic responses to a negative (<span class="math inline">\(\pi_{1}\)</span>) vs. positive wording of the question (<span class="math inline">\(\pi_{2}\)</span>).</p></li>
</ul>
The quantity of interest is then the population difference
<span class="math display">\[\begin{equation}\Delta=\pi_{2}-\pi_{1}.
\label{eq:Dp2sample}\end{equation}\]</span>
For a significance test of this, the null hypothesis will again be <span class="math inline">\(H_{0}:\; \Delta=0\)</span>, which is in this case equivalent to the hypothesis of equal proportions
<span class="math display">\[\begin{equation}H_{0}:\; \pi_{1} = \pi_{2}.
\label{eq:H0pD}\end{equation}\]</span>
<p>The null hypothesis thus claims that there is no association between the group variable <span class="math inline">\(X\)</span> and the dichotomous response variable <span class="math inline">\(Y\)</span>, while the alternative hypothesis (e.g.  the two-sided one <span class="math inline">\(H_{a}:\; \pi_{1}\ne \pi_{2}\)</span>, i.e. <span class="math inline">\(H_{a}:\; \Delta\ne 0\)</span>) implies that there is an association.</p>
The obvious estimates of <span class="math inline">\(\pi_{1}\)</span> and <span class="math inline">\(\pi_{2}\)</span> are the corresponding sample proportions <span class="math inline">\(\hat{\pi}_{1}\)</span> and <span class="math inline">\(\hat{\pi}_{2}\)</span>, calculated from samples of sizes <span class="math inline">\(n_{1}\)</span> and <span class="math inline">\(n_{2}\)</span> respectively, and the estimate of <span class="math inline">\(\Delta\)</span> is then
<span class="math display">\[\begin{equation}\hat{\Delta}=\hat{\pi}_{2} - \hat{\pi}_{1}.
\label{eq:Dhatpi}\end{equation}\]</span>
<p>This gives <span class="math inline">\(\hat{\Delta}=0.000284-0.000706=-0.000422\)</span> in Example 5.3 and <span class="math inline">\(\hat{\Delta}=0.364-0.279=0.085\)</span> in Example 5.4. In the samples, the proportion of polio cases is thus lower in the vaccinated group, and the proportion of optimistic answers is higher in response to a positively worded question. Note also that although the inference discussed below focuses on the difference of the proportions, for purely descriptive purposes we might prefer to use some other statistic, such as the ratio of the proportions. For example, the difference of 0.000422 in polio incidence between vaccine and control groups may seem small, because the proportions in both groups are small. A better idea of the the magnitude of the contrast is given by their ratio of <span class="math inline">\(0.000706/0.000284=2.49\)</span> (this is known as the <em>risk ratio</em>). In other words, the rate of polio infection in the unvaccinated group was two and a half times the rate in the vaccinated group.</p>
<p> The tests and confidence intervals discussed below are again based on the assumption that the relevant sampling distributions are approximately normal, which is true when the sample sizes <span class="math inline">\(n_{1}\)</span> and <span class="math inline">\(n_{2}\)</span> are large enough. The conditions for this are not very demanding: one rule of thumb states that the methods described in this section are reasonably valid if in both groups the number of observations with <span class="math inline">\(Y\)</span> having the value 1, and of ones with the value 0, are both more than 5. This condition is satisfied in both of the examples considered here.</p>
<p>The validity of the test, as well as the amount of information the data provide about <span class="math inline">\(\pi_{1}\)</span> and <span class="math inline">\(\pi_{2}\)</span> in general, thus depends not just on the overall sample sizes but on having enough observations of both values of <span class="math inline">\(Y\)</span>. The critical quantity is then the number of observations in the rarer category of <span class="math inline">\(Y\)</span>. In Example 5.3 this means the numbers of children diagnosed with polio, because the probability of polio was low in the study population. The numbers of eventual polio cases were 142 and 57 in the control and treatment groups respectively, so the rule of thumb stated above was satisfied. With such low probabilities of polio incidence, sufficient numbers of cases were achieved only by making the overall sample sizes <span class="math inline">\(n_{1}\)</span> and <span class="math inline">\(n_{2}\)</span> large enough. That is why the trial had to be very large, involving hundreds of thousands of participants.</p>
The standard error of <span class="math inline">\(\hat{\Delta}\)</span> is
<span class="math display">\[\begin{equation}\sigma_{\hat{\Delta}} =
\sqrt{
\frac{\pi_{2}(1-\pi_{2})}{n_{2}}
+\frac{\pi_{1}(1-\pi_{1})}{n_{1}}
}.
\label{eq:sigmaDpi}\end{equation}\]</span>
As in the one-sample case above, the best way to estimate this is different for a significance test than for a confidence interval. For a test, the standard error can be estimated under assumption that the null hypothesis (\ref{eq:H0pD}) is true, in which case the population proportion is the same in both groups. A good estimate of this common proportion, denoted below by <span class="math inline">\(\hat{\pi}\)</span>, is the proportion of observations with value 1 for <span class="math inline">\(Y\)</span> in the total sample of <span class="math inline">\(n_{1}+n_{2}\)</span> observations, pooling observations from both groups together; expressed in terms of the group-specific estimates, this is
<span class="math display">\[\begin{equation}\hat{\pi} = \frac{n_{1}\hat{\pi}_{1}+n_{2}\hat{\pi}_{2}}{n_{1}+n_{2}}.
\label{eq:phat2sample}\end{equation}\]</span>
Using this for both <span class="math inline">\(\pi_{1}\)</span> and <span class="math inline">\(\pi_{2}\)</span> in (\ref{eq:sigmaDpi}) gives the estimated standard error
<span class="math display">\[\begin{equation}\hat{\sigma}_{\hat{\Delta}}=
\sqrt{
\hat{\pi}(1-\hat{\pi}) \; \left(
\frac{1}{n_{2}}+
\frac{1}{n_{1}}
\right),
}
\label{eq:seDpi}\end{equation}\]</span>
<p>and using (@ref(eq:Dhatpi}) and ()–()–()–() and ( and  and ) from (\ref{eq:mu2)) leaves us with <span class="math inline">\(\beta_{3}\)</span>. In other words, <span class="math inline">\(\beta_{3}\)</span> is the change in expected value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_{3}\)</span> is increased by one unit, while keeping the values of <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> unchanged. The same result would obviously be obtained for <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>, and for models with any number of explanatory variables. Thus in general</p>
<ul>
<li>The regression coefficient of any explanatory variable in a multiple linear regression model shows the change in expected value of the response variable <span class="math inline">\(Y\)</span> when that explanatory variable is increased by one unit, while holding all other explanatory variables constant.</li>
</ul>
<p>When there is only one explanatory variable, the “while holding…” part is omitted and the interpretation becomes the one for simple linear regression in Section <a href="#ss-regression-simple-int"><strong>??</strong></a>.</p>
<p>This interpretation of the regression coefficients is obtained by “increasing by one unit” and “holding constant” values of explanatory variables by mathematical manipulations alone. It is thus true within the model even when the values of the explanatory variables are not and cannot actually be controlled and set at different values by the researcher. This, however, also implies that this appealing interpretation is a mathematical construction which does not automatically correspond to reality. In short, the interpretation of the regression coefficients is always mathematically true, but whether it is also an approximately correct description of an association in the real world depends on the appropriateness of the model for the data and study at hand. In some studies it is indeed possible to manipulate at least some explanatory variables, and corresponding regression models can then help to draw reasonably strong conclusions about associations between variables. Useful results can also be obtained in studies where no variables are in our control (so-called <em>observational studies</em>), as long as the model is selected carefully. This requires, in particular, that a linear model specification is adequate for the data, and that no crucially important explanatory variables have been omitted from the model.</p>
<p>In the IMR example, the estimated coefficients in Table <a href="#tab:t-imr-m2"><strong>??</strong></a> are interpreted as follows:</p>
<ul>
<li><p>Holding levels of Control of corruption and Income inequality constant, increasing School enrolment by one percentage point decreases expected IMR by 0.139 percentage points.</p></li>
<li><p>Holding levels of School enrolment and Income inequality constant, increasing Control of corruption by one unit decreases expected IMR by 0.046 percentage points.</p></li>
<li><p>Holding levels of School enrolment and Control of corruption constant, increasing Income inequality by one unit increases expected IMR by 0.055 percentage points.</p></li>
</ul>
<p>Instead of “holding constant”, we often talk about “controlling for” other variables in such statements. As before, it may be more convenient to express the interpretations in terms of other increments than one unit (e.g. ten units of the measure of Income inequality) by multiplying the coefficient by the correponding value.</p>
<p>The association between the response variable <span class="math inline">\(Y\)</span> and a particular explanatory variable <span class="math inline">\(X\)</span> described by the coefficient of <span class="math inline">\(X\)</span> in a multiple regression model is known as a <strong>partial association</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, controlling for the other explanatory variables in the model. This will often differ from the association estimated from a simple regression model for <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, because of the correlations between the control variables and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In the infant mortality example, the estimated effect of School enrolment was qualitatively unaffected by controlling for the other variables, and decreased in magnitude from -0.179 to -0.139.</p>
<div id="inference" class="section level4 unnumbered">
<h4>Inference</h4>
Inference for the regression coefficients in a multiple linear model differs from that for the simple model in interpretation but not in execution. Let <span class="math inline">\(\hat{\beta}_{j}\)</span> denote the estimated coefficient of an explanatory variable <span class="math inline">\(X_{j}\)</span> (where <span class="math inline">\(j\)</span> may be any of <span class="math inline">\(1,2,\dots,k\)</span>), and let <span class="math inline">\(\hat{\text{se}}(\hat{\beta}_{j})\)</span> denote its estimated standard error. The standard errors cannot now be calculated by hand, but they are routinely produced by computer packages and displayed as in Table <a href="#tab:t-imr-m2"><strong>??</strong></a>. A <span class="math inline">\(t\)</span>-test statistic for the null hypothesis discussed below is given by
<span class="math display">\[\begin{equation}t=\frac{\hat{\beta}_{j}}{\hat{\text{se}}(\hat{\beta}_{j})}.
\label{eq:tbeta-m}\end{equation}\]</span>
This is identical in form to statistic (\ref{eq:tbeta}) for the simple regression model. The corresponding null hypotheses are, however, subtly but crucially different in the two cases. In a multiple model, (\ref{eq:tbeta-m}) is a test statistic for the null hypothesis
<span class="math display">\[\begin{equation}H_{0}:\; \beta_{j}=0, \text{other regression coefficients
are unrestricted}
\label{eq:H0beta-m}\end{equation}\]</span>
<p>against the alternative hypothesis <span class="math display">\[H_{a}:\; \beta_{j}\ne0, \text{other regression coefficients
are unrestricted}.\]</span> Here the statement about “unrestricted” other parameters implies that neither hypothesis makes any claims about the values of other coefficients than <span class="math inline">\(\beta_{j}\)</span>, and these are allowed to have any values. The null hypothesis is a claim about the association between <span class="math inline">\(X_{j}\)</span> and <span class="math inline">\(Y\)</span> when the other explanatory variables are already included in the model. In other words, (\ref{eq:tbeta-m}) tests <span class="math display">\[\begin{aligned}
H_{0}:&amp; &amp; \text{There is no partial association between }
X_{j} \text{ and } Y,\\
&amp;&amp;  \text{controlling for the other explanatory
variables.}\end{aligned}\]</span></p>
<p>The sampling distribution of (\ref{eq:tbeta-m}) when the null hypothesis (\ref{eq:H0beta-m}) holds is a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-(k+1)\)</span> degrees of freedom, where <span class="math inline">\(k\)</span> is again the number of explanatory variables in the model. The test statistic and its <span class="math inline">\(P\)</span>-value from the <span class="math inline">\(t_{n-(k+1)}\)</span> distribution are shown in standard computer output, in a form similar to Table <a href="#tab:t-imr-m2"><strong>??</strong></a>.</p>
<p>It is important to note two things about test results for multiple regression models, such as those in Table <a href="#tab:t-imr-m2"><strong>??</strong></a>. First, (\ref{eq:H0beta-m}) implies that if the null hypothesis is not rejected, <span class="math inline">\(X_{j}\)</span> is not associated with <span class="math inline">\(Y\)</span>, <em>if</em> the other explanatory variables are already included in the model. We would typically react to this by removing <span class="math inline">\(X_{j}\)</span> from the model, while keeping the other variables in it. This is because of a general principle that models should usually be as simple (<em>parsimonious</em>) as possible, and not include variables which have no partial effect on the response variable. Second, the <span class="math inline">\(k\)</span> tests and <span class="math inline">\(P\)</span>-values actually refer to <span class="math inline">\(k\)</span> <em>different</em> hypotheses of the form (\ref{eq:H0beta-m}), one for each explanatory variable. This raises the question of what to do if, say, tests for two variables have large <span class="math inline">\(P\)</span>-values, suggesting that either of them could be removed from the model. The appropriate reaction is to remove one of the variables (perhaps the one with the larger <span class="math inline">\(P\)</span>-value) rather than both at once, and then see whether the other still remains nonsignificant (if so, it can then also be removed). This is part of the general area of <strong>model selection</strong>, principles and practice of which are mostly beyond the scope of this course; some further comments on it are given in Section <a href="c-probs.html#s-regression-rest">5.9</a>.</p>
<p>In the example shown in Table <a href="#tab:t-imr-m2"><strong>??</strong></a>, the <span class="math inline">\(P\)</span>-values are small for the tests for all of the coefficients. Each of the three explanatory variables thus has a significant effect on the response even after controlling for the other two, so none of the variables should be removed from the model.</p>
A confidence interval with confidence level <span class="math inline">\(1-\alpha\)</span> for any <span class="math inline">\(\beta_{j}\)</span> is given by
<span class="math display">\[\begin{equation}\hat{\beta}_{j} \pm t_{\alpha/2}^{(n-(k+1))} \,
\hat{\text{se}}(\hat{\beta}_{j}).
\label{eq:cibeta-m}\end{equation}\]</span>
<p>This is identical in form and interpretation to the interval (\ref{eq:cibeta}) for simple regression (except that the degrees of freedom are now <span class="math inline">\(df=n-(k+1)\)</span>), so no new issues arise. The confidence intervals for the coefficients in our example (where <span class="math inline">\(df=n-4=107\)</span> and <span class="math inline">\(t_{0.025}^{(107)}=1.98\)</span>) are shown in Table <a href="#tab:t-imr-m2"><strong>??</strong></a>.</p>
</div>
</div>
<div id="s-regression-dummies" class="section level2">
<h2><span class="header-section-number">5.8</span> Including categorical explanatory variables</h2>
<div id="ss-regression-dummies-def" class="section level3">
<h3><span class="header-section-number">5.8.1</span> Dummy variables</h3>
<p>Our models for Infant mortality rate so far did not include some more basic characteristics of the countries than school enrolment, corruption and income inequality. In particular, it seems desirable to control for the wealth of a country, which is likely to be correlated with both a health outcome like infant mortality and the other measures of development used as explanatory variables. We will do this by adding to the model the income level of the country, classified in the Global Civil Society Yearbook into three groups as Low, Middle or High income. Here one reason for considering income as a categorical variable is obviously to obtain an illustrative example for this section. However, using a variable like income in a grouped form is also more generally common practice. It also has the advantage that it is one way of dealing with cases where the effects on <span class="math inline">\(Y\)</span> of the corresponding continuous explanatory variable may be nonlinear.</p>
<p>Summary statistics in Table <a href="#tab:t-imrvars"><strong>??</strong></a> show that income group is associated with both IMR and the explanatory variables considered so far: countries with higher income tend to have lower levels of infant mortality, and higher school enrolment, less corruption and less income inequality than lower-income countries. It thus seems that controlling for income is potentially necessary, and may change the conclusions from the model.</p>
<p>Trying to add income level to the model confronts us with a new problem: how can a categorical explanatory variable like this be used in linear regression? This question is not limited to the current example, but is unavoidable in the social sciences. Even just the standard background variables such as sex, marital status, education level, party preference, employment status and region of residence considered in most individual-level studies are mostly categorical. Similarly, most survey data on attitudes and behaviour are collected in a categorical form, and even variables such as age or income which are originally continuous are often used in a grouped form. Categorical variables are thus ubiquitous in the social sciences, and it is essential to be able to use them also in regression models. How this is done is explained in this section, again illustrated with the infant mortality example. Section <a href="c-probs.html#ss-regression-dummies-example">5.8.2</a> then describes a different example for further illustration of the techniques.</p>
<p>The key to handling categorical explanatory variables is the use of dummy variables. A <strong>dummy variable</strong> (or <strong>indicator variable</strong>) is a variable with only two values, 0 and 1. Its value is 1 if a unit is in a particular category of a categorical variable, and 0 if it is not in that category. For example, we can define for each country the variable <span class="math display">\[D_{m}=\begin{cases}
1 &amp; \text{if Income level is ``Middle&#39;&#39;} \\
0 &amp; \text{otherwise.}
\end{cases}\]</span> This would typically be referred to as something like “dummy for middle income level”. Note that the label <span class="math inline">\(D_{m}\)</span> used here has no special significance, and was chosen simply to make it easy to remember. Dummy variables will be treated below as regular explanatory variables, and we could denote them as <span class="math inline">\(X\)</span>s just like all the others. A dummy for high income level is defined similarly as <span class="math display">\[D_{h}=\begin{cases}
1 &amp; \text{if Income level is ``High&#39;&#39;} \\
0 &amp; \text{otherwise.}
\end{cases}\]</span> The two variables <span class="math inline">\(D_{m}\)</span> and <span class="math inline">\(D_{h}\)</span> are enough to identify the income level of any country. If a country is in the middle-income group, the two dummies have the values <span class="math inline">\(D_{m}=1\)</span> and <span class="math inline">\(D_{h}=0\)</span> (as no country can be in two groups), and if it has high income, the dummies are <span class="math inline">\(D_{m}=0\)</span> and <span class="math inline">\(D_{h}=1\)</span>. For low-income countries, both <span class="math inline">\(D_{m}=0\)</span> and <span class="math inline">\(D_{h}=0\)</span>. There is thus no need to define a dummy for low income, because this category is identified by the two other dummies being both zero. The same is true in general: if a categorical variable has <span class="math inline">\(K\)</span> categories, only <span class="math inline">\(K-1\)</span> dummy variables are needed to identify the category of every unit. Note, in particular, that <em>dichotomous</em> variables with only two categories (<span class="math inline">\(K=2\)</span>) are fully identified by just one dummy variable. The category which is not given a dummy of its own is known as the <strong>reference category</strong> or <strong>baseline category</strong>. Any category can be the baseline, and this is usually chosen in a way to make interpretation (discussed below) convenient. The results of the model will be the same, whichever baseline is used.</p>
<p>Categorical variables are used as explanatory variables in regression models by including the dummy variables for them in the model. The results for this in our example are shown in Table <a href="c-probs.html#tab:t-imr-m3">5.4</a>. This requires no changes in the definition or estimation of the model, and the parameter estimates, standard errors and quantities for statistical inference are obtained exactly as before even when some of the explanatory variables are dummy variables. The only aspect which requires some further explanation is the interpretation of the coefficients of the dummy variables.</p>
<table>
<caption><span id="tab:t-imr-m3">Table 5.4: </span> Response variable: Infant Mortality Rate (%). Results for a linear regression model for Infant mortality rate in the Global Civil Society data, given the three explanatory variables in Table <a href="#tab:t-imr-m2"><strong>??</strong></a> and Income level in three groups. <span class="math inline">\(\hat{\sigma}=2.01\)</span>; <span class="math inline">\(R^{2}=0.753\)</span>; <span class="math inline">\(n=111\)</span>; <span class="math inline">\(df=105\)</span>.</caption>
<tbody>
<tr class="odd">
<td align="left">Explanatory variable</td>
<td align="right"></td>
<td align="right">Std. </td>
<td align="right"></td>
<td align="right"></td>
<td align="center">95 % Conf. </td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">Coefficient</td>
<td align="right">error</td>
<td align="right"><span class="math inline">\(t\)</span></td>
<td align="right"><span class="math inline">\(P\)</span>-value</td>
<td align="center">interval</td>
</tr>
<tr class="odd">
<td align="left">Constant</td>
<td align="right">12.00</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">School enrolment (%)</td>
<td align="right"><span class="math inline">\(-0.091\)</span></td>
<td align="right">0.016</td>
<td align="right"><span class="math inline">\(-5.69\)</span></td>
<td align="right"><span class="math inline">\(&lt;0.001\)</span></td>
<td align="center">$(-0.123;</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center">-0.059)$</td>
</tr>
<tr class="even">
<td align="left">Control of corruption</td>
<td align="right"><span class="math inline">\(-0.020\)</span></td>
<td align="right">0.011</td>
<td align="right"><span class="math inline">\(-1.75\)</span></td>
<td align="right"><span class="math inline">\(0.083\)</span></td>
<td align="center">$(-0.043;</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center">0.003)$</td>
</tr>
<tr class="even">
<td align="left">Income inequality</td>
<td align="right">0.080</td>
<td align="right">0.021</td>
<td align="right">3.75</td>
<td align="right"><span class="math inline">\(&lt;0.001\)</span></td>
<td align="center">(0.038; 0.122)</td>
</tr>
<tr class="odd">
<td align="left">Income level</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">(Reference group: Low)</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Middle</td>
<td align="right"><span class="math inline">\(-3.210\)</span></td>
<td align="right">0.631</td>
<td align="right"><span class="math inline">\(-5.09\)</span></td>
<td align="right"><span class="math inline">\(&lt;0.001\)</span></td>
<td align="center">$(-4.461;</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center">-1.958)$</td>
</tr>
<tr class="odd">
<td align="left">High</td>
<td align="right"><span class="math inline">\(-3.296\)</span></td>
<td align="right">1.039</td>
<td align="right"><span class="math inline">\(-3.17\)</span></td>
<td align="right">0.002</td>
<td align="center">$(-5.357;</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center">-1.235)$</td>
</tr>
</tbody>
</table>
<p>Recall that the regression coefficient of a continuous explanatory variable <span class="math inline">\(X\)</span> is the expected change in the response variable when <span class="math inline">\(X\)</span> is increased by one unit, holding all other explanatory variables constant. Exactly the same interpretation works for dummy variables, except that it is limited to the only one-unit increase possible for them, i.e. from 0 to 1. For example, consider two (hypothetical) countries with values 0 and 1 for the dummy <span class="math inline">\(D_{m}\)</span> for middle income, but with the same values for the three continuous explanatory variables. How about the other dummy variable <span class="math inline">\(D_{h}\)</span>, for high income? The interpretation requires that this too is held constant in the comparison. If this constant value was 1, it would not be possible for <span class="math inline">\(D_{m}\)</span> to be 1 because every country is in only one income group. Thus the only value at which <span class="math inline">\(D_{h}\)</span> can be held constant while <span class="math inline">\(D_{m}\)</span> is changed is 0, so that the comparison will be between a country with <span class="math inline">\(D_{m}=1, \, D_{h}=0\)</span> and one with <span class="math inline">\(D_{m}=0,\, D_{h}=0\)</span>, both with the same values of the other explanatory variables. In other words, the interpretation of the coefficient of <span class="math inline">\(D_{m}\)</span> refers to a comparison in expected value of <span class="math inline">\(Y\)</span> between a middle-income country and a country in the baseline category of low income, controlling for the other explanatory variables. The same applies to the coefficient of <span class="math inline">\(D_{h}\)</span>, and of dummy variables in general:</p>
<ul>
<li>The coefficient of a dummy variable for a particular level of a categorical explanatory variable is interpreted as the <em>difference</em> in the expected value of the response variable <span class="math inline">\(Y\)</span> between a unit with that level of the categorical variable and a unit in the baseline category, holding all other explanatory variables constant.</li>
</ul>
<p>Here the estimated coefficient of <span class="math inline">\(D_{m}\)</span> is <span class="math inline">\(-3.21\)</span>. In other words, comparing a middle-income country and a low-income country, both with the same levels of School enrolment, Control of corruption and Income inequality, the expected IMR is 3.21 percentage points lower in the middle-income country than in the low-income one. Similarly, a high-income country has an expected IMR 3.296 percentage points lower than a low-income one, other things being equal. The expected difference between the two non-reference levels is obtained as the difference of their coefficients (or by making one of them the reference level, as discussed below); here <span class="math inline">\(-3.296-(-3.210)=-0.086\)</span>, so a high-income country has an expected IMR 0.086 percentage points lower than a middle-income one, controlling for the other explanatory variables.</p>
<p>Predicted values are again obtained by substituting values for the explanatory variables, including appropriate zeros and ones for the dummy variables, into the estimated regression equation. For example, the predicted IMR for a country with School enrolment of 86 %, Control of corruption score of 50 and Income inequality of 40 is <span class="math display">\[\begin{aligned}
\hat{Y}&amp;=&amp;12.0-0.091\times 86-0.020\times 50+0.080\times 40-3.210\times
0-3.296\times 0\\
&amp;=&amp;
6.37 \text{for a low-income country, and }\\
\hat{Y}&amp;=&amp;12.0-0.091\times 86-0.020\times 50+0.080\times 40-3.210\times
1-3.296\times 0\\
&amp;=&amp;
6.37-3.21=3.16 \text{for a middle-income country,}\end{aligned}\]</span> with a difference of 3.21, as expected. Note how the constant term 12.0 sets the level for the baseline (low-income) group, and the coefficient <span class="math inline">\(-3.21\)</span> shows the change from that level when considering a middle-income country instead. Note also that we should again avoid unrealistic combinations of the variables in such predictions. For example, the above values would not be appropriate for high-income countries, because there are no such countries in these data with Control of corruption as low as 50.</p>
<p>The choice of the reference category does not affect the fitted model, and exactly the same results are obtained with any choice. For example, if high income is used as the reference category instead, the coefficients of the three continuous variables are unchanged from Table <a href="c-probs.html#tab:t-imr-m3">5.4</a>, and the coefficients of the dummy variables for low and middle incomes are 3.296 and 0.086 respectively. The conclusions from these are the same as above: controlling for the other explanatory variables, the difference in expected IMR is 3.296 between low and high-income, 0.086 between middle and high-income and <span class="math inline">\(3.296-0.086=3.210\)</span> between low and middle-income countries. Because the choice is arbitrary, the baseline level should be selected in whichever way is most convenient for stating the interpretation. If the categorical variable is ordinal (as it is here), it makes most sense for the baseline to be the first or last category. In other ways the dummy-variable treatment makes no distinction between nominal and ordinal categorical variables. Both are treated effectively as nominal in fitting the model, and information on any ordering of the categories is ignored.</p>
<p>Significance tests and confidence intervals are obtained for coefficients of dummy variables exactly as for any regression coefficients. Since the coefficient is in this case interpreted as an expected difference between a level of a categorical variable and the reference level, the null hypothesis of a zero coefficient is the hypothesis that there is no such difference. For example, Table <a href="c-probs.html#tab:t-imr-m3">5.4</a> shows that the coefficients of both the middle income and high income dummies are clearly significantly different from zero. This shows that, controlling for the other explanatory variables, expected infant mortality for both middle and high-income countries is different from that in low-income countries. The 95% confidence intervals in Table <a href="c-probs.html#tab:t-imr-m3">5.4</a> are intervals for this difference.</p>
<p>On the other hand, the coefficients of the two higher groups are very similar, which suggests that they may not be different from each other. This can be confirmed by fitting the same model with high income as the reference level, including dummies for low and middle groups. In this model (not shown here), the coefficient of the middle income dummy corresponds to the difference of the Middle and High groups. Its <span class="math inline">\(P\)</span>-value is 0.907 and 95% confidence interval <span class="math inline">\((-1.37; 1.54)\)</span>, so the difference is clearly not significant. This suggests that we could simplify the model further by combining the two higher groups and considering only two income groups, low vs. middle/high.</p>
<p>In cases like this where a categorical explanatory variable has more than two categories, <span class="math inline">\(t\)</span>-tests of individual coefficients are tests of hypotheses about no differences between individual categories, not the hypothesis that the variable has no effect overall. This is the hypothesis that the coefficients of the dummies for <em>all</em> of the categories are zero. This requires a slightly different test, which will not be considered here. In our example the low income category is so obviously different from the other two that it is clear that the hypothesis of no overall income effect would be rejected.</p>
<p>The main reason for including income group in the example was not to study income effects themselves (it is after all not all that surprising that infant mortality is highest in poor countries), but to control for them when examining partial associations between IMR and the other explanatory variables. These describe the estimated effects of these continuous variables when comparing countries with similar income levels. Comparing the results in Tables <a href="#tab:t-imr-m2"><strong>??</strong></a> and <a href="c-probs.html#tab:t-imr-m3">5.4</a>, it can be seen that the effect of School enrolment remains significant and negative (with higher enrolment associated with lower mortality), although its magnitude decreases somewhat after controlling for income group. Some but not all of its estimated effect in the first model is thus explained away by the fact that income is associated with both primary school enrolment and infant mortality, with richer countries having both higher enrolment and lower mortality.</p>
<p>The effect of Income inequality also remains significant in the larger model, even with a slightly increased coefficient. Countries with larger income inequality tend to have higher levels of infant mortality, even when we compare countries with similar levels of income. The effect of Control of corruption, on the other hand, is no longer significant in Table <a href="c-probs.html#tab:t-imr-m3">5.4</a>. This variable is strongly associated with income (as seen in Table <a href="#tab:t-imrvars"><strong>??</strong></a>), with the more corrupt countries typically being poor. Controlling for income, however, level of corruption appears to have little further effect on infant mortality. This also suggests that we might simplify the model by omitting the corruption variable.</p>
<p>One final remark on dummy variables establishes a connection to the techniques discussed in Chapter <a href="#c-means"><strong>??</strong></a>. There we described statistical inference for comparisons of the population means of a continuous response variable <span class="math inline">\(Y\)</span> between two groups, denoted 1 and 2. Suppose now that we fit a simple linear regression model for <span class="math inline">\(Y\)</span>, with a dummy variable for group 2 as the only explanatory variable. This gives exactly the same results as the two-sample <span class="math inline">\(t\)</span>-tests and confidence intervals (under the assumption of equal variances in the groups) in Section <a href="#s-means-inference"><strong>??</strong></a>. Related to the notation of that section, the coefficients from the model are <span class="math inline">\(\hat{\alpha}=\bar{Y}_{1}\)</span>, <span class="math inline">\(\hat{\beta}=\bar{Y}_{2}-\bar{Y}_{1}\)</span>, and <span class="math inline">\(\hat{\sigma}\)</span> from (\ref{eq:sigma-linreg}) is equal to (\ref{eq:sehatjoint}). Similarly, the standard error (\ref{eq:sebeta}) is the same as <span class="math inline">\(\hat{\sigma}_{\hat{\Delta}}\)</span> in (\ref{eq:seD2}), and the test statistic (\ref{eq:tbeta}) and confidence interval (\ref{eq:cibeta}) are identical with (\ref{eq:ztestmuDb}) and the <span class="math inline">\(t\)</span> distribution version of (\ref{eq:ciDmu2}) respectively.</p>
<p>The connection between linear regression and the two-sample <span class="math inline">\(t\)</span>-test is an illustration of how statistical methods are not a series of separate tricks for different situations, but a set of connected procedures unified by common principles. Whenever possible, methods for more complex problems have been created by extending those for simpler ones, and simple analyses are in turn special cases of more general ones. Although these connections are unfortunately often somewhat obscured by changes in language and notation, trying to understand them is very useful for effective learning of statistics.</p>
</div>
<div id="ss-regression-dummies-example" class="section level3">
<h3><span class="header-section-number">5.8.2</span> A second example</h3>
<p>Because the analysis of the models for infant mortality was presented piecemeal to accompany the introduction of different elements of linear regression, an overall picture of that example may have been difficult to discern. This section describes a different analysis in a more concise manner. It is particularly an illustration of the use of dummy variables, as most of the explanatory variables are categorical. The example concerns the relationship between minimum wage and employment, and uses data originally collected and analysed by David Card and Alan Krueger.<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> Most of the choices of analyses and variables considered here are based on those of Card and Krueger. Their article should be consulted for discussion and more detailed analyses.</p>
<p>A minimum wage of $5.05 per hour came into effect in the U.S. state of New Jersey on April 1 1992. This represented an increase from the previous, federally mandated minimum wage of $4.25 per hour. Conventional economic theory predicts that employers will react to such an increase by cutting their work force. Here the research hypothesis is thus that employment will be reduced among businesses affected by the new minimum wage. This can be addressed using suitable data, examining a statistical hypothesis of no association between measures of minimum wage increase and change of employment, controlling for other relevant explanatory variables.</p>
<p>Card and Krueger collected data for 410 fast food restaurants at two times, about one month before and eight months after the mininum wage increase came into effect in New Jersey. Only the 368 restaurants with known values of all the variables used in the analyses are included here. Of them, 268 were in New Jersey and had starting wages below $5.05 at the time of the first interview, so that these had to be increased to meet the new minimum wage. The remaining 100 restaurants provide a control group which was not affected by the change: 75 of them were in neighbouring eastern Pennsylvania where the minimum wage remained at $4.25, and 25 were in New Jersey but had starting wages of at least $5.05 even before the increase. The theoretical prediction is that the control group should experience a smaller negative employment change than the restaurants affected by the wage increase, i.e. employment in the control restaurants should not decrease or at least decrease less than in the affected restaurants. Card and Krueger argue that fast food restaurants provide a good population for examining the research question, because they employ many low-wage workers, generally comply with minimum wage legislation, do not receive tips which would complicate wage calculations, and are relatively easy to sample and interview.</p>
<p>The response variable considered here is the change between the two interviews in full-time equivalent employment at the restaurant, defined as the number of full-time workers (including managers) plus half the number of part-time workers. This will be called “Employment change” below. We consider two variables which indicate how the restaurant was affected by the minimum wage increase. The first is simply a dummy variable which is 1 for those New Jersey restaurants where wages needed to be raised because of the increase, and 0 for the other restaurants. These will be referred to as “Affected” and “Unaffected” restaurants respectively. The second variable is also 0 for the unaffected restaurants; for the affected ones, it is the proportion by which their previous starting wage had to be increased to meet the new minimum wage. For example, if the previous starting wage was the old minimum of $4.25, this “Wage gap” is <span class="math inline">\((5.05-4.25)/4.25=0.188\)</span>. Finally, we will also use information on the chain the restaurant belongs to (Burger King, Roy Rogers, Wendy’s or KFC) and whether it is owned by the parent company or the franchisee. These will be included in the analyses as partial control for other factors affecting the labour market, which might have had a differential impact on different types of restaurants over the study period. Summary statistics for the variables are shown in Table <a href="c-probs.html#tab:t-fastfood-descr">5.5</a>.</p>
<table>
<caption><span id="tab:t-fastfood-descr">Table 5.5: </span> Summary statistics for the variables considered in the minimum wage example of Section <a href="c-probs.html#ss-regression-dummies-example">5.8.2</a>. Mean employment change: Among unaffected restaurants: <span class="math inline">\(-2.93\)</span>; Among affected restaurants: <span class="math inline">\(+0.68\)</span>.</caption>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right">Minimum</td>
<td align="left">wage variable</td>
<td align="center">Response</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right">Affected</td>
<td align="left">Wage gap</td>
<td align="center">variable:</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left">(mean for</td>
<td align="center">Employment</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left">affected</td>
<td align="center">change</td>
</tr>
<tr class="odd">
<td align="left">Group</td>
<td align="right">%</td>
<td align="right"><span class="math inline">\((n)\)</span></td>
<td align="right">% (<span class="math inline">\(n\)</span>)</td>
<td align="left">restaurants)</td>
<td align="center">(mean)</td>
</tr>
<tr class="even">
<td align="left">Overall</td>
<td align="right">100</td>
<td align="right">(368)</td>
<td align="right">72.8 (268)</td>
<td align="left">0.115</td>
<td align="center"><span class="math inline">\(-0.30\)</span></td>
</tr>
<tr class="odd">
<td align="left">Ownership</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Franchisee</td>
<td align="right">64.7</td>
<td align="right">(238)</td>
<td align="right">71.8 (171)</td>
<td align="left">0.122</td>
<td align="center"><span class="math inline">\(-0.17\)</span></td>
</tr>
<tr class="odd">
<td align="left">Company</td>
<td align="right">35.3</td>
<td align="right">(130)</td>
<td align="right">74.6 (97)</td>
<td align="left">0.103</td>
<td align="center"><span class="math inline">\(-0.52\)</span></td>
</tr>
<tr class="even">
<td align="left">Chain</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Burger King</td>
<td align="right">41.0</td>
<td align="right">(151)</td>
<td align="right">73.5 (111)</td>
<td align="left">0.129</td>
<td align="center"><span class="math inline">\(+0.02\)</span></td>
</tr>
<tr class="even">
<td align="left">Roy Rogers</td>
<td align="right">24.7</td>
<td align="right">(91)</td>
<td align="right">72.5 (66)</td>
<td align="left">0.104</td>
<td align="center"><span class="math inline">\(-1.89\)</span></td>
</tr>
<tr class="odd">
<td align="left">Wendy’s</td>
<td align="right">13.0</td>
<td align="right">(48)</td>
<td align="right">60.4 (29)</td>
<td align="left">0.086</td>
<td align="center"><span class="math inline">\(-0.28\)</span></td>
</tr>
<tr class="even">
<td align="left">KFC</td>
<td align="right">21.2</td>
<td align="right">(78)</td>
<td align="right">79.5 (62)</td>
<td align="left">0.117</td>
<td align="center"><span class="math inline">\(+0.94\)</span></td>
</tr>
</tbody>
</table>
<hr />
<p>  Model (1)   Model (2)<br />
Coefficient (<span class="math inline">\(t\)</span>) Coefficient (<span class="math inline">\(t\)</span>) Variable (std error) <span class="math inline">\(P\)</span>-value (std error) <span class="math inline">\(P\)</span>-value Constant<br />
-2.63 -1.54 Affected by the increase 3.56 (3.50) — — (1.02) 0.001 — — Wage gap — — 15.88 (2.63) — — (6.04) 0.009 Ownership (vs. Franchisee)<br />
Company 0.22 (0.20) 0.43 (0.40) (1.06) 0.84 (1.07) 0.69 Chain (vs. Burger King)<br />
Roy Rogers -2.00 (-1.56) -1.84 (-1.43) (1.28) 0.12 (1.29) 0.15 Wendy’s 0.15 (0.11) 0.36 (0.24) (1.44) 0.92 (1.46) 0.81 KFC 0.64 (0.51) 0.81 (0.64) (1.25) 0.61 (1.26) 0.52</p>
<p><span class="math inline">\(R^{2}\)</span> 0.046 0.032 —————————————————————————————————————————————————-</p>
<p>: (#tab:t-fastfood-models) Response variable: Change in Full-time equivalent employment. Two fitted models for Employment change given exposure to minimum wage increase and control variables. See the text for further details.</p>
<p>Table <a href="#tab:t-fastfood-models">5.6</a> shows results for two linear regression models for Employment change, one using the dummy for affected restaurants and one using Wage gap as an explanatory variable. Both include the same dummy variables for ownership and chain of the restaurant. Consider first the model in column (1), which includes the dummy variable for affected restaurants. The estimated coefficient of this is 3.56, which is statistically significant (with <span class="math inline">\(t=3.50\)</span> and <span class="math inline">\(P=0.001\)</span>). This means that the estimated expected Employment change for the restaurants affected by the minimum wage increase was 3.56 full-time equivalent employees larger (in the positive direction) than for unaffected restaurants, controlling for the chain and type of ownership of the restaurant. This is the opposite of the theoretical prediction that the difference would be negative, due to the minimum wage increase leading to reductions of work force among affected restaurants but little change for the unaffected ones. In fact, the summary statistics in Table <a href="c-probs.html#tab:t-fastfood-descr">5.5</a> show (although without controlling for chain and ownership) that average employment actually <em>increased</em> in absolute terms among the affected restaurants, but decreased among the unaffected ones.</p>
<p>The coefficients of the control variables in Table <a href="#tab:t-fastfood-models">5.6</a> describe estimated differences between company-owned and franchisee-owned restaurants, and between Burger Kings and restaurants of other chains, controlling for the other variables in the model. All of these coefficients have high <span class="math inline">\(P\)</span>-values for both models, suggesting that the differences are small. In fact, the only one which is borderline significant, after all the other control dummies are successively removed from the model (not shown here), is that Employment change seems to have been more negative for Roy Rogers restaurants than for the rest. This side issue is not investigated in detail here. In any case, the control variables have little influence on the effect of the variable of main interest: if all the control dummies are removed from Model (1), the coefficient of the dummy variable for affected restaurants becomes 3.61 with a standard error of 1.01, little changed from the estimates in Table <a href="#tab:t-fastfood-models">5.6</a>. This is not entirely surprising, as the control variables are weakly associated with the variable of interest: as seen in Table <a href="c-probs.html#tab:t-fastfood-descr">5.5</a>, the proportions of affected restaurants are mostly fairly similar among restaurants of all chains and types of ownership.</p>
<p>In their article, Card and Krueger carefully explore (and confirm) the robustness of their findings by considering a series of variants of the analysis, with different choices of variables and sets of observations. This is done to try to rule out the possibility that the main conclusions are reliant on, and possibly biased by, some specific features of the data and variables in the initial analysis, such as missing data or measurement error. Such sensitivity analyses would be desirable in most social science contexts, where single definitely best form of analysis is rarely obvious. Here we will carry out a modest version of such an assessment by considering the Wage gap variable as an alternative measure of the impact of minimum wage increase, instead of a dummy for affected restaurants. This is a continuous variable, but one whose values are 0 for all unaffected restaurants and vary only among the affected ones. The logic of using Wage gap as an explanatory variable here is that Employment change could be expected to depend not only on <em>whether</em> a restaurant had to increase its starting wage to meet the new minimum wage, but also on <em>how large</em> that compulsory increase was.</p>
<p>The results for the second analysis are shown as Model (2) in Table <a href="#tab:t-fastfood-models">5.6</a>. The results are qualitatively the same as for Model (1), in that the coefficients of the control dummies are not significant, and that of Wage gap (which is 15.88) is significant and positive. The estimated employment change is thus again larger for affected than for unaffected restaurants, and their difference now even increases when the wage rise required from an affected restaurant increases. To compare these results more directly to Model (1), we can consider a comparison between an unaffected restaurant (with Wage gap 0) and an affected one with Wage gap equal to its mean among the affected restaurants, which is 0.115 (c.f. Table <a href="c-probs.html#tab:t-fastfood-descr">5.5</a>). The estimated difference in Employment change between them, controlling for ownership and chain, is <span class="math inline">\(0.115\times 15.88=1.83\)</span>, which is somewhat lower than the 3.56 estimated from model (1).</p>
<p>This example is also a good illustration of the limitations of the <span class="math inline">\(R^{2}\)</span> statistic. The <span class="math inline">\(R^{2}\)</span> values of 0.046 and 0.032 are very small, so over 95% of the observed variation in employment changes remains unexplained by the variation in the three explanatory variables. In other words, there are large differences in Employment change experienced even by affected or unaffected restaurants of the same chain and type of ownership. This would make <em>predicting</em> the employment change for a particular restaurant a fairly hopeless task with these explanatory variables. However, prediction is not the point here. The research question focuses on possible differences in <em>average</em> changes in employment, and finding such differences is of interest even if variation around the averages is large.</p>
<p>In summary, the analysis provides no support for the theoretical prediction that the restaurants affected by a minimum wage increase would experience a larger negative job change than control restaurants. In fact, there was a small but significant difference in the opposite direction in both models described here, and in all of the analyses considered by Card and Krueger. The authors propose a number of tentative explanations for this finding, but do not present any of them as definitive.</p>
</div>
</div>
<div id="s-regression-rest" class="section level2">
<h2><span class="header-section-number">5.9</span> Other issues in linear regression modelling</h2>
<p>The material in this chapter provides a reasonably self-contained introduction to linear regression models. However, it is not possible for a course like this to cover comprehensively all aspects of the models, so some topics have been described rather superficially and several have been omitted altogether. In this section we briefly discuss some of them. First, three previously unexplained small items in standard SPSS output are described. Second, a list of further topics in linear regression is given.</p>
<p>An example of SPSS output for linear regression models was given in Figure <a href="#fig:f-spss-linreg"><strong>??</strong></a>. Most parts of it have been explained above, but three have not been mentioned. These can be safely ignored, because each is of minor importance in most analyses. However, it is worth giving a brief explanation so as not to leave these three as mysteries:</p>
<ul>
<li><p>“Adjusted R Square” in the “<strong>Model Summary</strong>” table is a statistic defined as <span class="math inline">\(R^{2}_{adj}=[(n-1)R^{2}-k]/(n-k-1)\)</span>. This is most relevant in situations where the main purpose of the model is prediction of future observations of <span class="math inline">\(Y\)</span>. The population value of the <span class="math inline">\(R^{2}\)</span> statistic is then a key criterion of model selection. <span class="math inline">\(R^{2}_{adj}\)</span> is a better estimate of it than standard <span class="math inline">\(R^{2}\)</span>. Unlike <span class="math inline">\(R^{2}\)</span>, <span class="math inline">\(R^{2}_{adj}\)</span> does not always increase when new explanatory variables are added to the model. As a sample statistic, <span class="math inline">\(R^{2}_{adj}\)</span> does not have the same interpretation as the proportion of variation of <span class="math inline">\(Y\)</span> explained as standard <span class="math inline">\(R^{2}\)</span>.</p></li>
<li><p>The last two columns of the “<strong>ANOVA</strong>” (Analysis of Variance) table show the test statistic and <span class="math inline">\(P\)</span>-value for the so-called <span class="math inline">\(F\)</span>-test<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a>. The null hypothesis for this is that the regression coefficients of <em>all</em> the explanatory variables are zero, i.e. <span class="math inline">\(\beta_{1}=\beta_{2}=\dots=\beta_{k}=0\)</span>. In the case of simple regression (<span class="math inline">\(k=1\)</span>), this is equivalent to the <span class="math inline">\(t\)</span>-test for <span class="math inline">\(\beta=0\)</span>. In multiple regression, it implies that none of the explanatory variables have an effect on the response variable. In practice, this is rejected in most applications. Rejecting the hypothesis implies that at least one of the explanatory variables is associated with the response, but the test provides no help for identifying <em>which</em> of the individual partial effects are significant. The <span class="math inline">\(F\)</span>-test is thus usually largely irrelevant. More useful is an extended version of it (which is not included in the default output), which is used for hypotheses that a set of several regression coefficients (but not all of them) is zero. For example, this could be used in the example of Table <a href="c-probs.html#tab:t-imr-m3">5.4</a> to test if income level had no effect on IMR, i.e. if the coefficients of the dummies for <em>both</em> middle and high income were zero.</p></li>
<li><p>The “Standardized Coefficients/Beta” in the “<strong>Coefficients</strong>” table are defined as <span class="math inline">\((s_{xj}/s_{y})\hat{\beta}_{j}\)</span>, where <span class="math inline">\(\hat{\beta}_{j}\)</span> is the estimated coefficient of <span class="math inline">\(X_{j}\)</span>, and <span class="math inline">\(s_{xj}\)</span> and <span class="math inline">\(s_{y}\)</span> are sample standard deviations of <span class="math inline">\(X_{j}\)</span> and <span class="math inline">\(Y\)</span> respectively. This is equal to the correlation of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_{j}\)</span> when <span class="math inline">\(X_{j}\)</span> is the only explanatory variable in the model, but not otherwise. The standardized coefficient describes the expected change in <span class="math inline">\(Y\)</span> in units of its sample standard error, when <span class="math inline">\(X_{j}\)</span> is increased by one standard error <span class="math inline">\(s_{xj}\)</span>, holding other explanatory variables constant. The aim of this exercise is to obtain coefficients which are more directly comparable between different explanatory variables. Ultimately it refers to the question of <strong>relative importance</strong> of the explanatory variables, i.e. “Which of the explanatory variables in the model is the most important?” This is understandably of interest in many cases, often perhaps more so than any other aspect of the model. Unfortunately, however, relative importance is also one of the hardest questions in modelling, and one without a purely statistical solution. Despite their appealing title, standardized coefficients have problems of their own and do not provide a simple tool for judging relative importance. For example, their values depend not only on the strength of association described by <span class="math inline">\(\hat{\beta}_{j}\)</span> but also on the standard deviation <span class="math inline">\(s_{xj}\)</span>, which can be different in different samples.</p>
<p>Sensible comparisons of the magnitudes of expected changes in <span class="math inline">\(Y\)</span> in response to changes in individual explanatory variables can usually be presented even without reference to standardized coefficients, simply by using the usual coefficients <span class="math inline">\(\hat{\beta}_{j}\)</span> and carefully considering the effects of suitably chosen increments of the explanatory variables. In general, it is also worth bearing in mind that questions of relative importance are often conceptually troublesome, for example between explanatory variables with very different practical implications. For instance, suppose that we have fitted a model for a measure of the health status of a person, given the amount of physical exercise the person takes (which can be changed by him/herself), investment in preventive healthcare in the area where the person lives (which can be changed, but with more effort and not by the individual) and the person’s age (which cannot be manipulated at all). The values of the unstandardized or standardized coefficients of these explanatory variables can certainly be compared, but it is not clear what statements about the relative sizes of the effects of “increasing” them would really mean.</p></li>
</ul>
<p>A further course on linear regression (e.g. first half of MY452) will typically examine the topics covered on this course in more detail, and then go on to discuss further issues. Here we will give just a list of some such topics, in no particular order:</p>
<ul>
<li><p>Model <strong>diagnostics</strong> to examine whether a particular model appears to be adequate for the data. The residuals <span class="math inline">\(Y_{i}-\hat{Y}_{i}\)</span> are a key tool in this, and the most important graphical diagnostic is simply a scatterplot of the residuals against the fitted values <span class="math inline">\(\hat{Y}_{i}\)</span>. One task of diagnostics is to identify individual <strong>outliers</strong> and <strong>influential observations</strong> which have a substantial impact on the fitted model.</p></li>
<li><p>Modelling <strong>nonlinear effects</strong> of the explanatory variables. This is mostly done simply by including transformed values like squares <span class="math inline">\(X^{2}\)</span> or logarithms <span class="math inline">\(\log(X)\)</span> as explanatory variables in the model. It is sometimes also useful to transform the response variable, e.g. using <span class="math inline">\(\log(Y)\)</span> as the response instead of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Including <strong>interactions</strong> between explanatory variables in the model. This is achieved simply by including products of them as explanatory variables.</p></li>
<li><p>Identifying and dealing with problems caused by extremely high correlations between the explanatory variables, known as problems of <strong>multicollinearity</strong>.</p></li>
<li><p><strong>Model selection</strong> to identify the best sets of explanatory variables to be used in the model. This may employ both significance tests and other approaches.</p></li>
<li><p>Analysis of Variance (<strong>ANOVA</strong>) and Analysis of Covariance (<strong>ANCOVA</strong>), which are terms used for models involving only or mostly categorical explanatory variables, particularly in the context of randomized experiments. Many of these models can be fitted as standard linear regression models with appropriate use of dummy variables, but the conventional terminology and notation for ANOVA and ANCOVA are somewhat different from the ones used here.</p></li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="18">
<li id="fn18"><p>ESS Round 5: European Social Survey Round 5 Data (2010). Data file edition 2.0. Norwegian Social Science Data Services, Norway � Data Archive and distributor of ESS data. The full data can be obtained from <code>http://ess.nsd.uib.no/ess/round5/</code>.<a href="c-probs.html#fnref18">↩</a></p></li>
<li id="fn20"><p>The data can be obtained from <code>http://www3.norc.org/gss+website/</code>, which gives further information on the survey, including the full text of the questionnaires.<a href="c-probs.html#fnref20">↩</a></p></li>
<li id="fn21"><p>ESS Round 5: European Social Survey Round 5 Data (2010). Data file edition 2.0. Norwegian Social Science Data Services, Norway � Data Archive and distributor of ESS data. The full data can be obtained from <code>http://ess.nsd.uib.no/ess/round5/</code>.<a href="c-probs.html#fnref21">↩</a></p></li>
<li id="fn22"><p>Strictly speaking, the analysis should incorporate sampling weights (variable <em>DWEIGHT</em>) to adjust for different sampling probabilities for different types of respondents. Here the weights are ignored. Using them would not change the main conclusions for these variables.<a href="c-probs.html#fnref22">↩</a></p></li>
<li id="fn23"><p>Card, D. and Krueger, A. B. (1994). Minimum wages and employment: A case study of the fast-food industry in New Jersey and Pennsylvania. <em>The American Economic Review</em> <strong>84</strong>, 772–793.<a href="c-probs.html#fnref23">↩</a></p></li>
<li id="fn24"><p>The sampling distribution of this test is the <span class="math inline">\(F\)</span> distribution. The letter in both refers to Sir Ronald Fisher, the founder of modern statistics.<a href="c-probs.html#fnref24">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="c-tables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="c-3waytables.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/kbenoit/coursepack-bookdown/edit/master/08-MY451_8.rmd",
"text": null
},
"download": ["Coursepack-MY451.pdf", "Coursepack-MY451.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
